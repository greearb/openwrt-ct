From 7eee7588559968755e88931097726f1f06664b05 Mon Sep 17 00:00:00 2001
From: Ben Greear <greearb@candelatech.com>
Date: Mon, 30 Mar 2015 14:13:35 -0700
Subject: [PATCH 22/37] pktgen: Copy from 3.19 kernel.

Still needs changes from 3.17 forward applied
I believe.

Signed-off-by: Ben Greear <greearb@candelatech.com>
---
 net/core/pktgen.c | 2967 ++++++++++++++++++++++++++++++++++-------------------
 net/core/pktgen.h |  450 ++++++++
 2 files changed, 2379 insertions(+), 1038 deletions(-)
 create mode 100644 net/core/pktgen.h

diff --git a/net/core/pktgen.c b/net/core/pktgen.c
index 6e1e10f..ebc41c6 100644
--- a/net/core/pktgen.c
+++ b/net/core/pktgen.c
@@ -69,9 +69,8 @@
  * for running devices in the if_list and sends packets until count is 0 it
  * also the thread checks the thread->control which is used for inter-process
  * communication. controlling process "posts" operations to the threads this
- * way.
- * The if_list is RCU protected, and the if_lock remains to protect updating
- * of if_list, from "add_device" as it invoked from userspace (via proc write).
+ * way. The if_lock should be possible to remove when add/rem_device is merged
+ * into this too.
  *
  * By design there should only be *one* "controlling" process. In practice
  * multiple write accesses gives unpredictable result. Understood by "write"
@@ -116,9 +115,6 @@
  * command by Adit Ranadive <adit.262@gmail.com>
  *
  */
-
-#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
-
 #include <linux/sys.h>
 #include <linux/types.h>
 #include <linux/module.h>
@@ -135,7 +131,6 @@
 #include <linux/ioport.h>
 #include <linux/interrupt.h>
 #include <linux/capability.h>
-#include <linux/hrtimer.h>
 #include <linux/freezer.h>
 #include <linux/delay.h>
 #include <linux/timer.h>
@@ -162,6 +157,7 @@
 #include <net/checksum.h>
 #include <net/ipv6.h>
 #include <net/udp.h>
+#include <net/tcp.h>
 #include <net/ip6_checksum.h>
 #include <net/addrconf.h>
 #ifdef CONFIG_XFRM
@@ -171,305 +167,210 @@
 #include <asm/byteorder.h>
 #include <linux/rcupdate.h>
 #include <linux/bitops.h>
-#include <linux/io.h>
-#include <linux/timex.h>
-#include <linux/uaccess.h>
+#include <asm/io.h>
 #include <asm/dma.h>
+#include <asm/uaccess.h>
 #include <asm/div64.h>		/* do_div */
-
-#define VERSION	"2.75"
-#define IP_NAME_SZ 32
-#define MAX_MPLS_LABELS 16 /* This is the max label stack depth */
-#define MPLS_STACK_BOTTOM htonl(0x00000100)
-
-#define func_enter() pr_debug("entering %s\n", __func__);
-
-/* Device flag bits */
-#define F_IPSRC_RND   (1<<0)	/* IP-Src Random  */
-#define F_IPDST_RND   (1<<1)	/* IP-Dst Random  */
-#define F_UDPSRC_RND  (1<<2)	/* UDP-Src Random */
-#define F_UDPDST_RND  (1<<3)	/* UDP-Dst Random */
-#define F_MACSRC_RND  (1<<4)	/* MAC-Src Random */
-#define F_MACDST_RND  (1<<5)	/* MAC-Dst Random */
-#define F_TXSIZE_RND  (1<<6)	/* Transmit size is random */
-#define F_IPV6        (1<<7)	/* Interface in IPV6 Mode */
-#define F_MPLS_RND    (1<<8)	/* Random MPLS labels */
-#define F_VID_RND     (1<<9)	/* Random VLAN ID */
-#define F_SVID_RND    (1<<10)	/* Random SVLAN ID */
-#define F_FLOW_SEQ    (1<<11)	/* Sequential flows */
-#define F_IPSEC_ON    (1<<12)	/* ipsec on for flows */
-#define F_QUEUE_MAP_RND (1<<13)	/* queue map Random */
-#define F_QUEUE_MAP_CPU (1<<14)	/* queue map mirrors smp_processor_id() */
-#define F_NODE          (1<<15)	/* Node memory alloc*/
-#define F_UDPCSUM       (1<<16)	/* Include UDP checksum */
-#define F_NO_TIMESTAMP  (1<<17)	/* Don't timestamp packets (default TS) */
-
-/* Thread control flag bits */
-#define T_STOP        (1<<0)	/* Stop run */
-#define T_RUN         (1<<1)	/* Start run */
-#define T_REMDEVALL   (1<<2)	/* Remove all devs */
-#define T_REMDEV      (1<<3)	/* Remove one dev */
-
-/* Xmit modes */
-#define M_START_XMIT		0	/* Default normal TX */
-#define M_NETIF_RECEIVE 	1	/* Inject packets into stack */
-#define M_QUEUE_XMIT		2	/* Inject packet into qdisc */
-
-/* If lock -- protects updating of if_list */
-#define   if_lock(t)           mutex_lock(&(t->if_lock));
-#define   if_unlock(t)           mutex_unlock(&(t->if_lock));
-
-/* Used to help with determining the pkts on receive */
-#define PKTGEN_MAGIC 0xbe9be955
-#define PG_PROC_DIR "pktgen"
-#define PGCTRL	    "pgctrl"
-
-#define MAX_CFLOWS  65536
-
-#define VLAN_TAG_SIZE(x) ((x)->vlan_id == 0xffff ? 0 : 4)
-#define SVLAN_TAG_SIZE(x) ((x)->svlan_id == 0xffff ? 0 : 4)
-
-struct flow_state {
-	__be32 cur_daddr;
-	int count;
-#ifdef CONFIG_XFRM
-	struct xfrm_state *x;
+#include <asm/timex.h>
+#include <linux/sched.h> /* sched_clock() */
+#include "pktgen.h"
+#include <linux/ktime.h>
+
+#define USE_NQW_CALLBACK
+#ifdef USE_NQW_CALLBACK
+#  include <linux/if_vlan.h>
+#  include <linux/if_macvlan.h>
 #endif
-	__u32 flags;
-};
-
-/* flow flag bits */
-#define F_INIT   (1<<0)		/* flow has been initialized */
+#define VERSION  "pktgen v3.9-ben: Packet Generator for packet performance testing.\n"
 
-struct pktgen_dev {
-	/*
-	 * Try to keep frequent/infrequent used vars. separated.
-	 */
-	struct proc_dir_entry *entry;	/* proc file */
-	struct pktgen_thread *pg_thread;/* the owner */
-	struct list_head list;		/* chaining in the thread's run-queue */
-	struct rcu_head	 rcu;		/* freed by RCU */
+static int pg_net_id __read_mostly;
 
-	int running;		/* if false, the test will stop */
+static int use_rel_ts = 0;
 
-	/* If min != max, then we will either do a linear iteration, or
-	 * we will do a random selection from within the range.
-	 */
-	__u32 flags;
-	int xmit_mode;
-	int min_pkt_size;
-	int max_pkt_size;
-	int pkt_overhead;	/* overhead for MPLS, VLANs, IPSEC etc */
-	int nfrags;
-	int removal_mark;	/* non-zero => the device is marked for
-				 * removal by worker thread */
-
-	struct page *page;
-	u64 delay;		/* nano-seconds */
-
-	__u64 count;		/* Default No packets to send */
-	__u64 sofar;		/* How many pkts we've sent so far */
-	__u64 tx_bytes;		/* How many bytes we've transmitted */
-	__u64 errors;		/* Errors when trying to transmit, */
-
-	/* runtime counters relating to clone_skb */
-
-	__u32 clone_count;
-	int last_ok;		/* Was last skb sent?
-				 * Or a failed transmit of some sort?
-				 * This will keep sequence numbers in order
-				 */
-	ktime_t next_tx;
-	ktime_t started_at;
-	ktime_t stopped_at;
-	u64	idle_acc;	/* nano-seconds */
-
-	__u32 seq_num;
-
-	int clone_skb;		/*
-				 * Use multiple SKBs during packet gen.
-				 * If this number is greater than 1, then
-				 * that many copies of the same packet will be
-				 * sent before a new packet is allocated.
-				 * If you want to send 1024 identical packets
-				 * before creating a new packet,
-				 * set clone_skb to 1024.
-				 */
+#define REMOVE 1
+#define FIND   0
 
-	char dst_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-	char dst_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-	char src_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-	char src_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+static const char *version = VERSION;
 
-	struct in6_addr in6_saddr;
-	struct in6_addr in6_daddr;
-	struct in6_addr cur_in6_daddr;
-	struct in6_addr cur_in6_saddr;
-	/* For ranges */
-	struct in6_addr min_in6_daddr;
-	struct in6_addr max_in6_daddr;
-	struct in6_addr min_in6_saddr;
-	struct in6_addr max_in6_saddr;
+static struct pktgen_dev *__pktgen_NN_threads(const struct pktgen_net *pn,
+					      const char *ifname, int remove);
+static int pktgen_remove_device(struct pktgen_thread *t, struct pktgen_dev *i);
+static int pktgen_add_device(struct pktgen_thread *t, const char *ifname);
+static struct pktgen_dev *pktgen_find_dev(struct pktgen_thread *t, const char *ifname, bool exact);
+static int pktgen_device_event(struct notifier_block *, unsigned long, void *);
+static void pktgen_run_all_threads(struct pktgen_net *pn, int background);
+static void pktgen_reset_all_threads(struct pktgen_net *pn);
+static void pktgen_stop_all_threads_ifs(struct pktgen_net *pn);
 
-	/* If we're doing ranges, random or incremental, then this
-	 * defines the min/max for those ranges.
-	 */
-	__be32 saddr_min;	/* inclusive, source IP address */
-	__be32 saddr_max;	/* exclusive, source IP address */
-	__be32 daddr_min;	/* inclusive, dest IP address */
-	__be32 daddr_max;	/* exclusive, dest IP address */
-
-	__u16 udp_src_min;	/* inclusive, source UDP port */
-	__u16 udp_src_max;	/* exclusive, source UDP port */
-	__u16 udp_dst_min;	/* inclusive, dest UDP port */
-	__u16 udp_dst_max;	/* exclusive, dest UDP port */
-
-	/* DSCP + ECN */
-	__u8 tos;            /* six MSB of (former) IPv4 TOS
-				are for dscp codepoint */
-	__u8 traffic_class;  /* ditto for the (former) Traffic Class in IPv6
-				(see RFC 3260, sec. 4) */
+static void pktgen_stop(struct pktgen_thread *t);
+static void pg_reset_latency_counters(struct pktgen_dev* pkt_dev);
+static void pktgen_clear_counters(struct pktgen_dev *pkt_dev, int seq_too,
+				  const char* reason);
+static void pktgen_mark_device(const struct pktgen_net *pn, const char *ifname);
+static void clear_nqw_hook(struct pktgen_thread* t, struct net_device* dev);
+static int set_nqw_hook(struct pktgen_thread* t, struct net_device* dev, int gfp);
 
-	/* MPLS */
-	unsigned int nr_labels;	/* Depth of stack, 0 = no MPLS */
-	__be32 labels[MAX_MPLS_LABELS];
-
-	/* VLAN/SVLAN (802.1Q/Q-in-Q) */
-	__u8  vlan_p;
-	__u8  vlan_cfi;
-	__u16 vlan_id;  /* 0xffff means no vlan tag */
-
-	__u8  svlan_p;
-	__u8  svlan_cfi;
-	__u16 svlan_id; /* 0xffff means no svlan tag */
-
-	__u32 src_mac_count;	/* How many MACs to iterate through */
-	__u32 dst_mac_count;	/* How many MACs to iterate through */
-
-	unsigned char dst_mac[ETH_ALEN];
-	unsigned char src_mac[ETH_ALEN];
-
-	__u32 cur_dst_mac_offset;
-	__u32 cur_src_mac_offset;
-	__be32 cur_saddr;
-	__be32 cur_daddr;
-	__u16 ip_id;
-	__u16 cur_udp_dst;
-	__u16 cur_udp_src;
-	__u16 cur_queue_map;
-	__u32 cur_pkt_size;
-	__u32 last_pkt_size;
-
-	__u8 hh[14];
-	/* = {
-	   0x00, 0x80, 0xC8, 0x79, 0xB3, 0xCB,
-
-	   We fill in SRC address later
-	   0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
-	   0x08, 0x00
-	   };
-	 */
-	__u16 pad;		/* pad out the hh struct to an even 16 bytes */
+/* Module parameters, defaults. */
+static int pg_count_d __read_mostly = 1000;	  /* 1000 pkts by default */
+static int pg_delay_d __read_mostly = 0x7FFFFFFF; /* Don't run until someone sets a different delay. */
+static int pg_clone_skb_d __read_mostly;
+static int debug __read_mostly;
 
-	struct sk_buff *skb;	/* skb we are to transmit next, used for when we
-				 * are transmitting the same one multiple times
-				 */
-	struct net_device *odev; /* The out-going device.
-				  * Note that the device should have it's
-				  * pg_info pointer pointing back to this
-				  * device.
-				  * Set when the user specifies the out-going
-				  * device name (not when the inject is
-				  * started as it used to do.)
-				  */
-	char odevname[32];
-	struct flow_state *flows;
-	unsigned int cflows;	/* Concurrent flows (config) */
-	unsigned int lflow;		/* Flow length  (config) */
-	unsigned int nflows;	/* accumulated flows (stats) */
-	unsigned int curfl;		/* current sequenced flow (state)*/
-
-	u16 queue_map_min;
-	u16 queue_map_max;
-	__u32 skb_priority;	/* skb priority field */
-	unsigned int burst;	/* number of duplicated packets to burst */
-	int node;               /* Memory node */
+static DEFINE_MUTEX(pktgen_thread_lock);
 
-#ifdef CONFIG_XFRM
-	__u8	ipsmode;		/* IPSEC mode (config) */
-	__u8	ipsproto;		/* IPSEC type (config) */
-	__u32	spi;
-	struct dst_entry dst;
-	struct dst_ops dstops;
-#endif
-	char result[512];
+static struct notifier_block pktgen_notifier_block = {
+	.notifier_call = pktgen_device_event,
 };
 
-struct pktgen_hdr {
-	__be32 pgh_magic;
-	__be32 seq_num;
-	__be32 tv_sec;
-	__be32 tv_usec;
-};
+/*  This code works around the fact that do_div cannot handle two 64-bit
+    numbers, and regular 64-bit division doesn't work on x86 kernels.
+    --Ben
+*/
+
+#define PG_DIV 0
 
+/* This was emailed to LMKL by: Chris Caputo <ccaputo@alt.net>
+ * Function copied/adapted/optimized from:
+ *
+ *  nemesis.sourceforge.net/browse/lib/static/intmath/ix86/intmath.c.html
+ *
+ * Copyright 1994, University of Cambridge Computer Laboratory
+ * All Rights Reserved.
+ *
+ */
+static inline s64 divremdi3(s64 x, s64 y, int type)
+{
+	u64 a = (x < 0) ? -x : x;
+	u64 b = (y < 0) ? -y : y;
+	u64 res = 0, d = 1;
+
+	if (b > 0) {
+		while (b < a) {
+			b <<= 1;
+			d <<= 1;
+		}
+	}
 
-static unsigned int pg_net_id __read_mostly;
+	do {
+		if (a >= b) {
+			a -= b;
+			res += d;
+		}
+		b >>= 1;
+		d >>= 1;
+	}
+	while (d);
 
-struct pktgen_net {
-	struct net		*net;
-	struct proc_dir_entry	*proc_dir;
-	struct list_head	pktgen_threads;
-	bool			pktgen_exiting;
-};
+	if (PG_DIV == type) {
+		return (((x ^ y) & (1ll << 63)) == 0) ? res : -(s64) res;
+	} else {
+		return ((x & (1ll << 63)) == 0) ? a : -(s64) a;
+	}
+}
 
-struct pktgen_thread {
-	struct mutex if_lock;		/* for list of devices */
-	struct list_head if_list;	/* All device here */
-	struct list_head th_list;
-	struct task_struct *tsk;
-	char result[512];
+/* End of hacks to deal with 64-bit math on x86 */
 
-	/* Field for thread to receive "posted" events terminate,
-	   stop ifs etc. */
+/** Convert to micro-seconds */
+static inline __u64 ts_to_us(const struct timespec *ts)
+{
+	__u64 us = ts->tv_nsec / NSEC_PER_USEC;
+	us += ((__u64) ts->tv_sec) * 1000000ULL;
+	return us;
+}
 
-	u32 control;
-	int cpu;
+static inline __s64 pg_div(__s64 n, __u32 base)
+{
+	if (n < 0) {
+		__u64 tmp = -n;
+		do_div(tmp, base);
+		/* printk("pktgen: pg_div, n: %llu  base: %d  rv: %llu\n",
+		   n, base, tmp); */
+		return -tmp;
+	}
+	else {
+		__u64 tmp = n;
+		do_div(tmp, base);
+		/* printk("pktgen: pg_div, n: %llu  base: %d  rv: %llu\n",
+		   n, base, tmp); */
+		return tmp;
+	}
+}
 
-	wait_queue_head_t queue;
-	struct completion start_done;
-	struct pktgen_net *net;
-};
+#if 0
+static inline __u64 pg_div64(__u64 n, __u64 base)
+{
+	__u64 tmp = n;
+/*
+ * How do we know if the architecture we are running on
+ * supports division with 64 bit base?
+ *
+ */
+#if defined(__sparc_v9__) || defined(__powerpc64__) || defined(__alpha__) || defined(__x86_64__) || defined(__ia64__)
 
-#define REMOVE 1
-#define FIND   0
+	do_div(tmp, base);
+#else
+	tmp = divremdi3(n, base, PG_DIV);
+#endif
+	return tmp;
+}
+#endif
 
-static const char version[] =
-	"Packet Generator for packet performance testing. "
-	"Version: " VERSION "\n";
+static inline __u64 getCurUs(void)
+{
+	struct timespec ts;
+	getnstimeofday(&ts);
+	return ts_to_us(&ts);
+}
 
-static int pktgen_remove_device(struct pktgen_thread *t, struct pktgen_dev *i);
-static int pktgen_add_device(struct pktgen_thread *t, const char *ifname);
-static struct pktgen_dev *pktgen_find_dev(struct pktgen_thread *t,
-					  const char *ifname, bool exact);
-static int pktgen_device_event(struct notifier_block *, unsigned long, void *);
-static void pktgen_run_all_threads(struct pktgen_net *pn);
-static void pktgen_reset_all_threads(struct pktgen_net *pn);
-static void pktgen_stop_all_threads_ifs(struct pktgen_net *pn);
 
-static void pktgen_stop(struct pktgen_thread *t);
-static void pktgen_clear_counters(struct pktgen_dev *pkt_dev);
+/* Since the machine booted. */
+static __u64 getRelativeCurNs(void) {
+	if (!use_rel_ts) {
+		struct timespec ts;
+		getnstimeofday(&ts);
+		return timespec_to_ns(&ts);
+	}
+	else {
+		/* Seems you must disable pre-empt to call sched_clock. --Ben */
+		unsigned long flags;
+		__u64 rv;
+		local_irq_save(flags);
+		rv = sched_clock();
+		local_irq_restore(flags);
+		return rv;
+	}
+}
 
-/* Module parameters, defaults. */
-static int pg_count_d __read_mostly = 1000;
-static int pg_delay_d __read_mostly;
-static int pg_clone_skb_d  __read_mostly;
-static int debug  __read_mostly;
+/* old include end */
 
-static DEFINE_MUTEX(pktgen_thread_lock);
 
-static struct notifier_block pktgen_notifier_block = {
-	.notifier_call = pktgen_device_event,
-};
+static void timestamp_skb(struct pktgen_dev* pkt_dev, struct pktgen_hdr* pgh) {
+	if (pkt_dev->flags & F_NO_TIMESTAMP) {
+		pgh->tv_hi = 0;
+		pgh->tv_lo = 0;
+		return;
+	}
+
+	if (pkt_dev->flags & F_USE_REL_TS) {
+		__u64 now = getRelativeCurNs();
+		__u32 hi = (now >> 32);
+		__u32 lo = now;
+		pgh->tv_hi = htonl(hi);
+		pgh->tv_lo = htonl(lo);
+	}
+	else {
+		struct timespec ts;
+		s64 n;
+		__u32 hi;
+		__u32 lo;
+		getnstimeofday(&ts);
+		n = timespec_to_ns(&ts);
+		hi = n >> 32;
+		lo = n;
+		pgh->tv_hi = htonl(hi);
+		pgh->tv_lo = htonl(lo);
+	}
+}
 
 /*
  * /proc handling functions
@@ -482,8 +383,8 @@ static int pgctrl_show(struct seq_file *seq, void *v)
 	return 0;
 }
 
-static ssize_t pgctrl_write(struct file *file, const char __user *buf,
-			    size_t count, loff_t *ppos)
+static ssize_t pgctrl_write(struct file *file, const char __user * buf,
+			    size_t count, loff_t * ppos)
 {
 	char data[128];
 	struct pktgen_net *pn = net_generic(current->nsproxy->net_ns, pg_net_id);
@@ -506,13 +407,14 @@ static ssize_t pgctrl_write(struct file *file, const char __user *buf,
 		pktgen_stop_all_threads_ifs(pn);
 
 	else if (!strcmp(data, "start"))
-		pktgen_run_all_threads(pn);
-
+		pktgen_run_all_threads(pn, 0);
+	/* Run in the background. */
+	else if (!strcmp(data, "bg_start"))
+		pktgen_run_all_threads(pn, 1);
 	else if (!strcmp(data, "reset"))
 		pktgen_reset_all_threads(pn);
-
 	else
-		return -EINVAL;
+		printk(KERN_WARNING "pktgen: Unknown command: %s\n", data);
 
 	return count;
 }
@@ -522,6 +424,149 @@ static int pgctrl_open(struct inode *inode, struct file *file)
 	return single_open(file, pgctrl_show, PDE_DATA(inode));
 }
 
+static int pg_populate_report(struct pktgen_dev_report* rpt, struct pktgen_dev* pkt_dev) {
+	int i;
+
+	memset(rpt, 0, sizeof(*rpt));
+	rpt->api_version = 1;
+	rpt->flags = pkt_dev->flags;
+	if (!pkt_dev->running)
+		rpt->flags |= (F_PG_STOPPED);
+	strncpy(rpt->thread_name, pkt_dev->pg_thread->tsk->comm, 32);
+	strncpy(rpt->interface_name, pkt_dev->ifname, 32);
+	rpt->min_pkt_size = pkt_dev->min_pkt_size;
+	rpt->max_pkt_size = pkt_dev->max_pkt_size;
+	rpt->clone_skb = pkt_dev->clone_skb;
+	rpt->conn_id = pkt_dev->conn_id;
+	rpt->peer_conn_id = pkt_dev->peer_conn_id;
+	rpt->peer_clone_skb = pkt_dev->peer_clone_skb;
+	rpt->nfrags = pkt_dev->nfrags;
+
+	strncpy(rpt->dst_min, pkt_dev->dst_min, IP_NAME_SZ);
+	strncpy(rpt->dst_max, pkt_dev->dst_max, IP_NAME_SZ);
+	strncpy(rpt->src_min, pkt_dev->src_min, IP_NAME_SZ);
+	strncpy(rpt->src_max, pkt_dev->src_max, IP_NAME_SZ);
+
+	memcpy(&rpt->in6_saddr, &pkt_dev->in6_saddr, sizeof(struct in6_addr));
+	memcpy(&rpt->in6_daddr, &pkt_dev->in6_daddr, sizeof(struct in6_addr));
+
+	/* For ranges */
+	memcpy(&rpt->min_in6_daddr, &pkt_dev->min_in6_daddr, sizeof(struct in6_addr));
+	memcpy(&rpt->max_in6_daddr, &pkt_dev->max_in6_daddr, sizeof(struct in6_addr));
+	memcpy(&rpt->min_in6_saddr, &pkt_dev->min_in6_saddr, sizeof(struct in6_addr));
+	memcpy(&rpt->max_in6_saddr, &pkt_dev->max_in6_saddr, sizeof(struct in6_addr));
+
+	/* If we're doing ranges, random or incremental, then this
+	 * defines the min/max for those ranges.
+	 */
+	rpt->saddr_min = pkt_dev->saddr_min;
+	rpt->saddr_max = pkt_dev->saddr_max;
+	rpt->daddr_min = pkt_dev->daddr_min;
+	rpt->daddr_max = pkt_dev->daddr_max;
+
+	rpt->udp_src_min = pkt_dev->udp_src_min;
+	rpt->udp_src_max = pkt_dev->udp_src_max;
+	rpt->udp_dst_min = pkt_dev->udp_dst_min;
+	rpt->udp_dst_max = pkt_dev->udp_dst_max;
+
+	/* MPLS */
+	rpt->nr_labels = pkt_dev->nr_labels;	/* Depth of stack, 0 = no MPLS */
+	for (i = 0; i<MAX_MPLS_LABELS; i++) {
+		rpt->labels[i] = pkt_dev->labels[i];
+	}
+
+	rpt->src_mac_count = pkt_dev->src_mac_count;
+	rpt->dst_mac_count = pkt_dev->dst_mac_count;
+
+	memcpy(&rpt->dst_mac, &pkt_dev->dst_mac, ETH_ALEN);
+	memcpy(&rpt->src_mac, &pkt_dev->src_mac, ETH_ALEN);
+
+	rpt->nflows = pkt_dev->nflows;
+	rpt->cflows = pkt_dev->cflows;
+	rpt->lflow = pkt_dev->lflow;
+
+	rpt->delay_ns = pkt_dev->delay_ns;
+	rpt->count = pkt_dev->count;  /* Default No packets to send */
+	rpt->sofar = pkt_dev->sofar;  /* How many pkts we've sent so far */
+	rpt->tx_bytes = pkt_dev->tx_bytes; /* How many bytes we've transmitted */
+	rpt->tx_bytes_ll = pkt_dev->tx_bytes_ll; /* How many bytes we've transmitted, including framing */
+	rpt->errors = pkt_dev->errors;    /* Errors when trying to transmit, pkts will be re-sent */
+
+	/* Fields relating to receiving pkts */
+	rpt->avg_latency = pkt_dev->avg_latency; /* in micro-seconds */
+	rpt->min_latency = pkt_dev->min_latency;
+	rpt->max_latency = pkt_dev->max_latency;
+	for (i = 0; i<LAT_BUCKETS_MAX; i++) {
+		rpt->latency_bkts[i] = pkt_dev->latency_bkts[i];
+	}
+	rpt->running_jitter = pkt_dev->running_jitter / 1024;
+	rpt->burst = pkt_dev->burst;
+	rpt->pkts_rcvd_since_clear_lat = pkt_dev->pkts_rcvd_since_clear_lat;
+	rpt->total_lat = pkt_dev->total_lat;
+
+        rpt->ooo_rcvd = pkt_dev->ooo_rcvd;
+        rpt->pkts_rcvd = pkt_dev->pkts_rcvd;
+	rpt->rx_crc_failed = pkt_dev->rx_crc_failed;
+        rpt->dup_rcvd = pkt_dev->dup_rcvd;
+        rpt->bytes_rcvd = pkt_dev->bytes_rcvd;
+        rpt->bytes_rcvd_ll = pkt_dev->bytes_rcvd_ll;
+	rpt->pkts_rcvd_wrong_conn = pkt_dev->pkts_rcvd_wrong_conn;
+        rpt->seq_gap_rcvd = pkt_dev->seq_gap_rcvd;
+	rpt->non_pg_pkts_rcvd = pkt_dev->non_pg_pkts_rcvd;
+	return 0;
+}; /* populate report */
+
+
+long pktgen_proc_ioctl(struct file* file, unsigned int cmd,
+                      unsigned long arg) {
+        int err = 0;
+        struct pktgen_ioctl_info args;
+        struct pktgen_dev* pkt_dev = NULL;
+	struct pktgen_net *pn = net_generic(current->nsproxy->net_ns, pg_net_id);
+
+        if (copy_from_user(&args, (void*)arg, sizeof(args))) {
+                return -EFAULT;
+        }
+
+        /* Null terminate the names */
+        args.thread_name[31] = 0;
+        args.interface_name[31] = 0;
+
+        /* printk("pktgen:  thread_name: %s  interface_name: %s\n",
+         *        args.thread_name, args.interface_name);
+         */
+
+        switch (cmd) {
+         case GET_PKTGEN_INTERFACE_INFO: {
+		 mutex_lock(&pktgen_thread_lock);
+                 pkt_dev = __pktgen_NN_threads(pn, args.interface_name, FIND);
+                 if (pkt_dev) {
+			 pg_populate_report(&(args.report), pkt_dev);
+			 if (copy_to_user((void*)(arg), &args, sizeof(args))) {
+				 printk("ERROR: pktgen:  copy_to_user failed.\n");
+				 err = -EFAULT;
+			 }
+			 else {
+				 err = 0;
+			 }
+		 }
+		 else {
+			 printk("ERROR: pktgen:  Could not find interface -:%s:-\n",
+				args.interface_name);
+			 err = -ENODEV;
+		 }
+		 mutex_unlock(&pktgen_thread_lock);
+                 break;
+         }
+         default:
+		printk("%s: Unknown pktgen IOCTL: %x \n", __FUNCTION__,
+			cmd);
+		return -EINVAL;
+        }
+
+        return err;
+}/* pktgen_proc_ioctl */
+
 static const struct file_operations pktgen_fops = {
 	.owner   = THIS_MODULE,
 	.open    = pgctrl_open,
@@ -529,23 +574,29 @@ static const struct file_operations pktgen_fops = {
 	.llseek  = seq_lseek,
 	.write   = pgctrl_write,
 	.release = single_release,
+        .unlocked_ioctl   = pktgen_proc_ioctl,
 };
 
 static int pktgen_if_show(struct seq_file *seq, void *v)
 {
 	const struct pktgen_dev *pkt_dev = seq->private;
-	ktime_t stopped;
-	u64 idle;
+	__u64 sa;
+	__u64 stopped;
+	__u64 now = getCurUs();
+	int i;
+	struct netdev_queue *txq;
 
 	seq_printf(seq,
-		   "Params: count %llu  min_pkt_size: %u  max_pkt_size: %u\n",
+		   "Params: count %llu  min_pkt_size: %u  max_pkt_size: %u conn_id: %u  peer_conn_id: %u\n",
 		   (unsigned long long)pkt_dev->count, pkt_dev->min_pkt_size,
-		   pkt_dev->max_pkt_size);
+		   pkt_dev->max_pkt_size, pkt_dev->conn_id, pkt_dev->peer_conn_id);
 
 	seq_printf(seq,
-		   "     frags: %d  delay: %llu  clone_skb: %d  ifname: %s\n",
-		   pkt_dev->nfrags, (unsigned long long) pkt_dev->delay,
-		   pkt_dev->clone_skb, pkt_dev->odevname);
+		   "     frags: %d  delay: %lluns  clone_skb: %d  peer_clone_skb: %d ifname: %s\n",
+		   pkt_dev->nfrags,
+		   (unsigned long long)pkt_dev->delay_ns,
+		   pkt_dev->clone_skb, pkt_dev->peer_clone_skb,
+		   pkt_dev->ifname);
 
 	seq_printf(seq, "     flows: %u flowlen: %u\n", pkt_dev->cflows,
 		   pkt_dev->lflow);
@@ -567,14 +618,12 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 			   &pkt_dev->min_in6_saddr, &pkt_dev->max_in6_saddr,
 			   &pkt_dev->in6_daddr,
 			   &pkt_dev->min_in6_daddr, &pkt_dev->max_in6_daddr);
-	} else {
-		seq_printf(seq,
-			   "     dst_min: %s  dst_max: %s\n",
-			   pkt_dev->dst_min, pkt_dev->dst_max);
+
+	} else
 		seq_printf(seq,
-			   "     src_min: %s  src_max: %s\n",
-			   pkt_dev->src_min, pkt_dev->src_max);
-	}
+			   "     dst_min: %s  dst_max: %s\n     src_min: %s  src_max: %s\n",
+			   pkt_dev->dst_min, pkt_dev->dst_max, pkt_dev->src_min,
+			   pkt_dev->src_max);
 
 	seq_puts(seq, "     src_mac: ");
 
@@ -586,8 +635,7 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 	seq_printf(seq, "%pM\n", pkt_dev->dst_mac);
 
 	seq_printf(seq,
-		   "     udp_src_min: %d  udp_src_max: %d"
-		   "  udp_dst_min: %d  udp_dst_max: %d\n",
+		   "     udp_src_min: %d  udp_src_max: %d  udp_dst_min: %d  udp_dst_max: %d\n",
 		   pkt_dev->udp_src_min, pkt_dev->udp_src_max,
 		   pkt_dev->udp_dst_min, pkt_dev->udp_dst_max);
 
@@ -603,21 +651,23 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 				   i == pkt_dev->nr_labels-1 ? "\n" : ", ");
 	}
 
-	if (pkt_dev->vlan_id != 0xffff)
+	if (pkt_dev->vlan_id != 0xffff) {
 		seq_printf(seq, "     vlan_id: %u  vlan_p: %u  vlan_cfi: %u\n",
-			   pkt_dev->vlan_id, pkt_dev->vlan_p,
-			   pkt_dev->vlan_cfi);
+			   pkt_dev->vlan_id, pkt_dev->vlan_p, pkt_dev->vlan_cfi);
+	}
 
-	if (pkt_dev->svlan_id != 0xffff)
+	if (pkt_dev->svlan_id != 0xffff) {
 		seq_printf(seq, "     svlan_id: %u  vlan_p: %u  vlan_cfi: %u\n",
-			   pkt_dev->svlan_id, pkt_dev->svlan_p,
-			   pkt_dev->svlan_cfi);
+			   pkt_dev->svlan_id, pkt_dev->svlan_p, pkt_dev->svlan_cfi);
+	}
 
-	if (pkt_dev->tos)
+	if (pkt_dev->tos) {
 		seq_printf(seq, "     tos: 0x%02x\n", pkt_dev->tos);
+	}
 
-	if (pkt_dev->traffic_class)
+	if (pkt_dev->traffic_class) {
 		seq_printf(seq, "     traffic_class: 0x%02x\n", pkt_dev->traffic_class);
+	}
 
 	if (pkt_dev->burst > 1)
 		seq_printf(seq, "     burst: %d\n", pkt_dev->burst);
@@ -625,11 +675,6 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 	if (pkt_dev->node >= 0)
 		seq_printf(seq, "     node: %d\n", pkt_dev->node);
 
-	if (pkt_dev->xmit_mode == M_NETIF_RECEIVE)
-		seq_puts(seq, "     xmit_mode: netif_receive\n");
-	else if (pkt_dev->xmit_mode == M_QUEUE_XMIT)
-		seq_puts(seq, "     xmit_mode: xmit_queue\n");
-
 	seq_puts(seq, "     Flags: ");
 
 	if (pkt_dev->flags & F_IPV6)
@@ -665,6 +710,15 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 	if (pkt_dev->flags & F_QUEUE_MAP_CPU)
 		seq_puts(seq,  "QUEUE_MAP_CPU  ");
 
+	if (pkt_dev->flags & F_PEER_LOCAL)
+		seq_puts(seq,  "PEER_LOCAL  ");
+
+	if (pkt_dev->flags & F_USE_REL_TS)
+		seq_puts(seq,  "USE_REL_TS  ");
+
+	if (pkt_dev->flags & F_TCP)
+		seq_puts(seq,  "TCP  ");
+
 	if (pkt_dev->cflows) {
 		if (pkt_dev->flags & F_FLOW_SEQ)
 			seq_puts(seq,  "FLOW_SEQ  "); /*in sequence flows*/
@@ -697,21 +751,66 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 
 	seq_puts(seq, "\n");
 
-	/* not really stopped, more like last-running-at */
-	stopped = pkt_dev->running ? ktime_get() : pkt_dev->stopped_at;
-	idle = pkt_dev->idle_acc;
-	do_div(idle, NSEC_PER_USEC);
+	sa = pkt_dev->started_at;
+	stopped = pkt_dev->stopped_at;
+	if (pkt_dev->running)
+		stopped = now;	/* not really stopped, more like last-running-at */
 
 	seq_printf(seq,
-		   "Current:\n     pkts-sofar: %llu  errors: %llu\n",
+		   "Current:\n     tx-pkts: %llu  tx-errors: %llu tx-cn: %llu  tx-drop: %llu\n",
 		   (unsigned long long)pkt_dev->sofar,
-		   (unsigned long long)pkt_dev->errors);
+		   (unsigned long long)pkt_dev->errors,
+		   (unsigned long long)pkt_dev->xmit_cn,
+		   (unsigned long long)pkt_dev->xmit_dropped);
+
+	seq_printf(seq, "    tx-bytes: %llu(%llu)\n",
+		   (unsigned long long)pkt_dev->tx_bytes,
+		   (unsigned long long)pkt_dev->tx_bytes_ll);
+
+
+	txq = netdev_get_tx_queue(pkt_dev->odev, pkt_dev->cur_queue_map);
+	seq_printf(seq,
+		   "     odev: %s  tx-q-map: %i  txq: %p  q-stopped-or-frozen: %i carrier: %i\n",
+		   pkt_dev->odevname, pkt_dev->cur_queue_map, txq,
+		   netif_xmit_frozen_or_stopped(txq),
+		   netif_carrier_ok(pkt_dev->odev));
+
+	seq_printf(seq,
+		   "     rx-pkts: %llu  rx-crc-failed: %llu rx-bytes: %llu(%llu) rx-wrong-conn: %llu alloc_skbs: %llu  oom_alloc_skbs: %llu\n",
+		   (unsigned long long)pkt_dev->pkts_rcvd,
+		   (unsigned long long)pkt_dev->rx_crc_failed,
+		   (unsigned long long)pkt_dev->bytes_rcvd,
+		   (unsigned long long)pkt_dev->bytes_rcvd_ll,
+		   (unsigned long long)pkt_dev->pkts_rcvd_wrong_conn,
+		   (unsigned long long)pkt_dev->allocated_skbs,
+		   (unsigned long long)pkt_dev->oom_on_alloc_skb);
+
 
 	seq_printf(seq,
-		   "     started: %lluus  stopped: %lluus idle: %lluus\n",
-		   (unsigned long long) ktime_to_us(pkt_dev->started_at),
-		   (unsigned long long) ktime_to_us(stopped),
-		   (unsigned long long) idle);
+		   "     blocked: %s  next-tx-ns: %llu (%lli)\n     started: %lluus  stopped: %lluus idle: %lluns\n",
+		   pkt_dev->tx_blocked ? "TRUE" : "false",
+		   (unsigned long long)pkt_dev->next_tx_ns,
+		   (long long)(pkt_dev->next_tx_ns - getRelativeCurNs()),
+		   (unsigned long long)sa,
+		   (unsigned long long)stopped,
+		   (unsigned long long)pkt_dev->idle_acc_ns);
+	seq_printf(seq,
+		   "     nanodelays: %llu  sleeps: %llu  queue_stopped: %llu  tx-early: %llu\n",
+		   (unsigned long long)pkt_dev->nanodelays,
+		   (unsigned long long)pkt_dev->sleeps,
+		   (unsigned long long)pkt_dev->queue_stopped,
+		   (unsigned long long)pkt_dev->req_tx_early);
+
+	seq_printf(seq,
+		   "     Total-Latency: %lli   Total-pkts-since-latency-clear: %llu  Avg-Jitter: %hu\n",
+		   pkt_dev->total_lat, pkt_dev->pkts_rcvd_since_clear_lat, pkt_dev->running_jitter / 1024);
+	seq_printf(seq,
+		   "     Latency(us): %i - %i - %i [",
+		   pkt_dev->min_latency, pkt_dev->avg_latency, pkt_dev->max_latency);
+	for (i = 0; i<LAT_BUCKETS_MAX; i++)
+		seq_printf(seq, "%llu ", pkt_dev->latency_bkts[i]);
+	seq_printf(seq, "]\n     Neg-Latency-Fixups (PEER_LOCAL only): %llu\n",
+		   pkt_dev->neg_latency);
 
 	seq_printf(seq,
 		   "     seq_num: %d  cur_dst_mac_offset: %d  cur_src_mac_offset: %d\n",
@@ -742,8 +841,7 @@ static int pktgen_if_show(struct seq_file *seq, void *v)
 }
 
 
-static int hex32_arg(const char __user *user_buffer, unsigned long maxlen,
-		     __u32 *num)
+static int hex32_arg(const char __user *user_buffer, unsigned long maxlen, __u32 *num)
 {
 	int i = 0;
 	*num = 0;
@@ -788,8 +886,8 @@ static int count_trail_chars(const char __user * user_buffer,
 	return i;
 }
 
-static long num_arg(const char __user *user_buffer, unsigned long maxlen,
-				unsigned long *num)
+static long num_arg(const char __user * user_buffer, unsigned long maxlen,
+		    unsigned long *num)
 {
 	int i;
 	*num = 0;
@@ -863,7 +961,7 @@ static ssize_t pktgen_if_write(struct file *file,
 			       const char __user * user_buffer, size_t count,
 			       loff_t * offset)
 {
-	struct seq_file *seq = file->private_data;
+	struct seq_file *seq = (struct seq_file *)file->private_data;
 	struct pktgen_dev *pkt_dev = seq->private;
 	int i, max, len;
 	char name[16], valstr[32];
@@ -875,14 +973,14 @@ static ssize_t pktgen_if_write(struct file *file,
 	pg_result = &(pkt_dev->result[0]);
 
 	if (count < 1) {
-		pr_warn("wrong command format\n");
+		printk(KERN_WARNING "pktgen: wrong command format\n");
 		return -EINVAL;
 	}
 
 	max = count;
 	tmp = count_trail_chars(user_buffer, max);
 	if (tmp < 0) {
-		pr_warn("illegal format\n");
+		printk(KERN_WARNING "pktgen: illegal format\n");
 		return tmp;
 	}
 	i = tmp;
@@ -890,9 +988,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	/* Read variable name */
 
 	len = strn_len(&user_buffer[i], sizeof(name) - 1);
-	if (len < 0)
+	if (len < 0) {
 		return len;
-
+	}
 	memset(name, 0, sizeof(name));
 	if (copy_from_user(name, &user_buffer[i], len))
 		return -EFAULT;
@@ -911,15 +1009,15 @@ static ssize_t pktgen_if_write(struct file *file,
 		if (copy_from_user(tb, user_buffer, copy))
 			return -EFAULT;
 		tb[copy] = 0;
-		pr_debug("%s,%lu  buffer -:%s:-\n",
-			 name, (unsigned long)count, tb);
+		pr_debug("%s,%lu  buffer -:%s:-\n", name,
+			 (unsigned long)count, tb);
 	}
 
 	if (!strcmp(name, "min_pkt_size")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value < 14 + 20 + 8)
 			value = 14 + 20 + 8;
@@ -934,9 +1032,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "max_pkt_size")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value < 14 + 20 + 8)
 			value = 14 + 20 + 8;
@@ -953,9 +1051,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "pkt_size")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value < 14 + 20 + 8)
 			value = 14 + 20 + 8;
@@ -970,9 +1068,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "debug")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		debug = value;
 		sprintf(pg_result, "OK: debug=%u", debug);
@@ -981,64 +1079,69 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "frags")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		pkt_dev->nfrags = value;
 		sprintf(pg_result, "OK: frags=%u", pkt_dev->nfrags);
 		return count;
 	}
-	if (!strcmp(name, "delay")) {
-		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
-			return len;
 
-		i += len;
-		if (value == 0x7FFFFFFF)
-			pkt_dev->delay = ULLONG_MAX;
-		else
-			pkt_dev->delay = (u64)value;
+	/* This is basically a flush..causes new skb to be generated, regardless of
+	 * current clone count.
+	 */
+	if (!strcmp(name, "force_new_skb")) {
+		/* If this is our first flush, then we just started, and we need to set up
+		 * the dup_since_incr to work properly.
+		 */
+		if (!pkt_dev->flushed_already) {
+			pkt_dev->dup_since_incr = pkt_dev->peer_clone_skb - 1;
+			pkt_dev->flushed_already = 1;
+		}
+		pkt_dev->force_new_skb = 1;
+		sprintf(pg_result, "OK: Forcing new SKB.\n");
 
-		sprintf(pg_result, "OK: delay=%llu",
-			(unsigned long long) pkt_dev->delay);
 		return count;
 	}
-	if (!strcmp(name, "rate")) {
+
+	if (!strcmp(name, "delay")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
-		if (!value)
-			return len;
-		pkt_dev->delay = pkt_dev->min_pkt_size*8*NSEC_PER_USEC/value;
-		if (debug)
-			pr_info("Delay set at: %llu ns\n", pkt_dev->delay);
 
-		sprintf(pg_result, "OK: rate=%lu", value);
-		return count;
-	}
-	if (!strcmp(name, "ratep")) {
-		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
-			return len;
+		/* If we are quiescing, then force delay to be quiesce value:
+		 * Maybe user-space hasn't noticed yet.
+		 */
+		if ((pkt_dev->count != 0) && (pkt_dev->sofar >= pkt_dev->count)) {
+			value = 0x7FFFFFFF;
+		}
 
-		i += len;
-		if (!value)
-			return len;
-		pkt_dev->delay = NSEC_PER_SEC/value;
-		if (debug)
-			pr_info("Delay set at: %llu ns\n", pkt_dev->delay);
+		/* If we are going from quiesce to running, We want to start with a new
+		 * SKB right now, instead of waiting for multi-skb to take affect.
+		 */
+		if ((pkt_dev->delay_ns == 0x7FFFFFFF) &&
+		    value != 0x7FFFFFFF)
+			pkt_dev->force_new_skb = 1;
+
+		pkt_dev->delay_ns = value;
+		if ((getRelativeCurNs() + pkt_dev->delay_ns) < pkt_dev->next_tx_ns) {
+			pkt_dev->next_tx_ns = getRelativeCurNs() + pkt_dev->delay_ns;
+		}
+
+		/* Break out of sleep loop if we were in it. */
+		pkt_dev->accum_delay_ns = 0;
 
-		sprintf(pg_result, "OK: rate=%lu", value);
+		sprintf(pg_result, "OK: delay=%lluns", (unsigned long long)pkt_dev->delay_ns);
 		return count;
 	}
 	if (!strcmp(name, "udp_src_min")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value != pkt_dev->udp_src_min) {
 			pkt_dev->udp_src_min = value;
@@ -1049,9 +1152,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "udp_dst_min")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value != pkt_dev->udp_dst_min) {
 			pkt_dev->udp_dst_min = value;
@@ -1062,9 +1165,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "udp_src_max")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value != pkt_dev->udp_src_max) {
 			pkt_dev->udp_src_max = value;
@@ -1075,9 +1178,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "udp_dst_max")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value != pkt_dev->udp_dst_max) {
 			pkt_dev->udp_dst_max = value;
@@ -1086,13 +1189,35 @@ static ssize_t pktgen_if_write(struct file *file,
 		sprintf(pg_result, "OK: udp_dst_max=%u", pkt_dev->udp_dst_max);
 		return count;
 	}
+        if (!strcmp(name, "conn_id")) {
+                len = num_arg(&user_buffer[i], 10, &value);
+                if (len < 0) {
+                        return len;
+                }
+                i += len;
+                pkt_dev->conn_id = value;
+
+                sprintf(pg_result, "OK: conn_id=%d", pkt_dev->conn_id);
+                return count;
+        }
+        if (!strcmp(name, "peer_conn_id")) {
+                len = num_arg(&user_buffer[i], 10, &value);
+                if (len < 0) {
+                        return len;
+                }
+                i += len;
+                pkt_dev->peer_conn_id = value;
+
+                sprintf(pg_result, "OK: peer_conn_id=%d", pkt_dev->peer_conn_id);
+                return count;
+        }
+
 	if (!strcmp(name, "clone_skb")) {
 		len = num_arg(&user_buffer[i], 10, &value);
 		if (len < 0)
 			return len;
 		if ((value > 0) &&
-		    ((pkt_dev->xmit_mode == M_NETIF_RECEIVE) ||
-		     !(pkt_dev->odev->priv_flags & IFF_TX_SKB_SHARING)))
+		    (!(pkt_dev->odev->priv_flags & IFF_TX_SKB_SHARING)))
 			return -ENOTSUPP;
 		i += len;
 		pkt_dev->clone_skb = value;
@@ -1100,11 +1225,22 @@ static ssize_t pktgen_if_write(struct file *file,
 		sprintf(pg_result, "OK: clone_skb=%d", pkt_dev->clone_skb);
 		return count;
 	}
-	if (!strcmp(name, "count")) {
+	if (!strcmp(name, "peer_clone_skb")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
+		}
+		i += len;
+		pkt_dev->peer_clone_skb = value;
 
+		sprintf(pg_result, "OK: peer_clone_skb=%d", pkt_dev->peer_clone_skb);
+		return count;
+	}
+	if (!strcmp(name, "count")) {
+		len = num_arg(&user_buffer[i], 10, &value);
+		if (len < 0) {
+			return len;
+		}
 		i += len;
 		pkt_dev->count = value;
 		sprintf(pg_result, "OK: count=%llu",
@@ -1113,9 +1249,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "src_mac_count")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (pkt_dev->src_mac_count != value) {
 			pkt_dev->src_mac_count = value;
@@ -1127,9 +1263,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "dst_mac_count")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (pkt_dev->dst_mac_count != value) {
 			pkt_dev->dst_mac_count = value;
@@ -1146,9 +1282,7 @@ static ssize_t pktgen_if_write(struct file *file,
 
 		i += len;
 		if ((value > 1) &&
-		    ((pkt_dev->xmit_mode == M_QUEUE_XMIT) ||
-		     ((pkt_dev->xmit_mode == M_START_XMIT) &&
-		     (!(pkt_dev->odev->priv_flags & IFF_TX_SKB_SHARING)))))
+		    (!(pkt_dev->odev->priv_flags & IFF_TX_SKB_SHARING)))
 			return -ENOTSUPP;
 		pkt_dev->burst = value < 1 ? 1 : value;
 		sprintf(pg_result, "OK: burst=%d", pkt_dev->burst);
@@ -1173,69 +1307,27 @@ static ssize_t pktgen_if_write(struct file *file,
 			sprintf(pg_result, "ERROR: node not possible");
 		return count;
 	}
-	if (!strcmp(name, "xmit_mode")) {
+	if (!strcmp(name, "flag")) {
 		char f[32];
-
 		memset(f, 0, 32);
 		len = strn_len(&user_buffer[i], sizeof(f) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		if (copy_from_user(f, &user_buffer[i], len))
 			return -EFAULT;
 		i += len;
+		if (strcmp(f, "IPSRC_RND") == 0)
+			pkt_dev->flags |= F_IPSRC_RND;
 
-		if (strcmp(f, "start_xmit") == 0) {
-			pkt_dev->xmit_mode = M_START_XMIT;
-		} else if (strcmp(f, "netif_receive") == 0) {
-			/* clone_skb set earlier, not supported in this mode */
-			if (pkt_dev->clone_skb > 0)
-				return -ENOTSUPP;
+		else if (strcmp(f, "!IPSRC_RND") == 0)
+			pkt_dev->flags &= ~F_IPSRC_RND;
 
-			pkt_dev->xmit_mode = M_NETIF_RECEIVE;
+		else if (strcmp(f, "TXSIZE_RND") == 0)
+			pkt_dev->flags |= F_TXSIZE_RND;
 
-			/* make sure new packet is allocated every time
-			 * pktgen_xmit() is called
-			 */
-			pkt_dev->last_ok = 1;
-
-			/* override clone_skb if user passed default value
-			 * at module loading time
-			 */
-			pkt_dev->clone_skb = 0;
-		} else if (strcmp(f, "queue_xmit") == 0) {
-			pkt_dev->xmit_mode = M_QUEUE_XMIT;
-			pkt_dev->last_ok = 1;
-		} else {
-			sprintf(pg_result,
-				"xmit_mode -:%s:- unknown\nAvailable modes: %s",
-				f, "start_xmit, netif_receive\n");
-			return count;
-		}
-		sprintf(pg_result, "OK: xmit_mode=%s", f);
-		return count;
-	}
-	if (!strcmp(name, "flag")) {
-		char f[32];
-		memset(f, 0, 32);
-		len = strn_len(&user_buffer[i], sizeof(f) - 1);
-		if (len < 0)
-			return len;
-
-		if (copy_from_user(f, &user_buffer[i], len))
-			return -EFAULT;
-		i += len;
-		if (strcmp(f, "IPSRC_RND") == 0)
-			pkt_dev->flags |= F_IPSRC_RND;
-
-		else if (strcmp(f, "!IPSRC_RND") == 0)
-			pkt_dev->flags &= ~F_IPSRC_RND;
-
-		else if (strcmp(f, "TXSIZE_RND") == 0)
-			pkt_dev->flags |= F_TXSIZE_RND;
-
-		else if (strcmp(f, "!TXSIZE_RND") == 0)
-			pkt_dev->flags &= ~F_TXSIZE_RND;
+		else if (strcmp(f, "!TXSIZE_RND") == 0)
+			pkt_dev->flags &= ~F_TXSIZE_RND;
 
 		else if (strcmp(f, "IPDST_RND") == 0)
 			pkt_dev->flags |= F_IPDST_RND;
@@ -1252,6 +1344,20 @@ static ssize_t pktgen_if_write(struct file *file,
 		else if (strcmp(f, "UDPDST_RND") == 0)
 			pkt_dev->flags |= F_UDPDST_RND;
 
+		else if ((strcmp(f, "UDPCSUM") == 0) ||
+			 (strcmp(f, "CSUM") == 0))
+			pkt_dev->flags |= F_UDPCSUM;
+
+		else if ((strcmp(f, "!UDPCSUM") == 0) ||
+			 (strcmp(f, "!CSUM") == 0))
+			pkt_dev->flags &= ~F_UDPCSUM;
+
+		else if (strcmp(f, "NO_TIMESTAMP") == 0)
+			pkt_dev->flags |= F_NO_TIMESTAMP;
+
+		else if (strcmp(f, "!NO_TIMESTAMP") == 0)
+			pkt_dev->flags &= ~F_NO_TIMESTAMP;
+
 		else if (strcmp(f, "!UDPDST_RND") == 0)
 			pkt_dev->flags &= ~F_UDPDST_RND;
 
@@ -1299,6 +1405,26 @@ static ssize_t pktgen_if_write(struct file *file,
 
 		else if (strcmp(f, "!QUEUE_MAP_CPU") == 0)
 			pkt_dev->flags &= ~F_QUEUE_MAP_CPU;
+
+		else if (strcmp(f, "PEER_LOCAL") == 0)
+			pkt_dev->flags |= F_PEER_LOCAL;
+
+		else if (strcmp(f, "!PEER_LOCAL") == 0)
+			pkt_dev->flags &= ~F_PEER_LOCAL;
+
+		else if (strcmp(f, "USE_REL_TS") == 0) {
+			if (pkt_dev->running && !(pkt_dev->flags & F_USE_REL_TS)) {
+				if (!use_rel_ts++)
+					pkt_dev->next_tx_ns = getRelativeCurNs();
+			}
+			pkt_dev->flags |= F_USE_REL_TS;
+		}
+		else if (strcmp(f, "!USE_REL_TS") == 0) {
+			if (pkt_dev->running && (pkt_dev->flags & F_USE_REL_TS))
+				if (use_rel_ts--)
+					pkt_dev->next_tx_ns = getRelativeCurNs();
+			pkt_dev->flags &= ~F_USE_REL_TS;
+		}
 #ifdef CONFIG_XFRM
 		else if (strcmp(f, "IPSEC") == 0)
 			pkt_dev->flags |= F_IPSEC_ON;
@@ -1313,19 +1439,14 @@ static ssize_t pktgen_if_write(struct file *file,
 		else if (strcmp(f, "!NODE_ALLOC") == 0)
 			pkt_dev->flags &= ~F_NODE;
 
-		else if (strcmp(f, "UDPCSUM") == 0)
-			pkt_dev->flags |= F_UDPCSUM;
-
-		else if (strcmp(f, "!UDPCSUM") == 0)
-			pkt_dev->flags &= ~F_UDPCSUM;
-
-		else if (strcmp(f, "NO_TIMESTAMP") == 0)
-			pkt_dev->flags |= F_NO_TIMESTAMP;
+		else if (strcmp(f, "TCP") == 0)
+			pkt_dev->flags |= F_TCP;
 
-		else if (strcmp(f, "!NO_TIMESTAMP") == 0)
-			pkt_dev->flags &= ~F_NO_TIMESTAMP;
+		else if (strcmp(f, "!TCP") == 0)
+			pkt_dev->flags &= ~F_TCP;
 
 		else {
+			printk("pktgen: Flag -:%s:- unknown\n", f);
 			sprintf(pg_result,
 				"Flag -:%s:- unknown\nAvailable flags, (prepend ! to un-set flag):\n%s",
 				f,
@@ -1345,8 +1466,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "dst_min") || !strcmp(name, "dst")) {
 		len = strn_len(&user_buffer[i], sizeof(pkt_dev->dst_min) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
+		}
 
 		if (copy_from_user(buf, &user_buffer[i], len))
 			return -EFAULT;
@@ -1365,9 +1487,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "dst_max")) {
 		len = strn_len(&user_buffer[i], sizeof(pkt_dev->dst_max) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 
 		if (copy_from_user(buf, &user_buffer[i], len))
 			return -EFAULT;
@@ -1476,9 +1598,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "src_min")) {
 		len = strn_len(&user_buffer[i], sizeof(pkt_dev->src_min) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		if (copy_from_user(buf, &user_buffer[i], len))
 			return -EFAULT;
 		buf[len] = 0;
@@ -1496,9 +1618,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "src_max")) {
 		len = strn_len(&user_buffer[i], sizeof(pkt_dev->src_max) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		if (copy_from_user(buf, &user_buffer[i], len))
 			return -EFAULT;
 		buf[len] = 0;
@@ -1516,9 +1638,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	}
 	if (!strcmp(name, "dst_mac")) {
 		len = strn_len(&user_buffer[i], sizeof(valstr) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		memset(valstr, 0, sizeof(valstr));
 		if (copy_from_user(valstr, &user_buffer[i], len))
 			return -EFAULT;
@@ -1527,15 +1649,14 @@ static ssize_t pktgen_if_write(struct file *file,
 			return -EINVAL;
 		/* Set up Dest MAC */
 		ether_addr_copy(&pkt_dev->hh[0], pkt_dev->dst_mac);
-
 		sprintf(pg_result, "OK: dstmac %pM", pkt_dev->dst_mac);
 		return count;
 	}
 	if (!strcmp(name, "src_mac")) {
 		len = strn_len(&user_buffer[i], sizeof(valstr) - 1);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		memset(valstr, 0, sizeof(valstr));
 		if (copy_from_user(valstr, &user_buffer[i], len))
 			return -EFAULT;
@@ -1544,22 +1665,27 @@ static ssize_t pktgen_if_write(struct file *file,
 			return -EINVAL;
 		/* Set up Src MAC */
 		ether_addr_copy(&pkt_dev->hh[6], pkt_dev->src_mac);
-
 		sprintf(pg_result, "OK: srcmac %pM", pkt_dev->src_mac);
 		return count;
 	}
 
 	if (!strcmp(name, "clear_counters")) {
-		pktgen_clear_counters(pkt_dev);
+		pktgen_clear_counters(pkt_dev, 0, "proc-write");
 		sprintf(pg_result, "OK: Clearing counters.\n");
 		return count;
 	}
 
+	if (!strcmp(name, "clear_latencies")) {
+		pg_reset_latency_counters(pkt_dev);
+		sprintf(pg_result, "OK: Clearing latency.\n");
+		return count;
+	}
+
 	if (!strcmp(name, "flows")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value > MAX_CFLOWS)
 			value = MAX_CFLOWS;
@@ -1568,6 +1694,7 @@ static ssize_t pktgen_if_write(struct file *file,
 		sprintf(pg_result, "OK: flows=%u", pkt_dev->cflows);
 		return count;
 	}
+
 #ifdef CONFIG_XFRM
 	if (!strcmp(name, "spi")) {
 		len = num_arg(&user_buffer[i], 10, &value);
@@ -1580,11 +1707,12 @@ static ssize_t pktgen_if_write(struct file *file,
 		return count;
 	}
 #endif
+
 	if (!strcmp(name, "flowlen")) {
 		len = num_arg(&user_buffer[i], 10, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		pkt_dev->lflow = value;
 		sprintf(pg_result, "OK: flowlen=%u", pkt_dev->lflow);
@@ -1593,9 +1721,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "queue_map_min")) {
 		len = num_arg(&user_buffer[i], 5, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		pkt_dev->queue_map_min = value;
 		sprintf(pg_result, "OK: queue_map_min=%u", pkt_dev->queue_map_min);
@@ -1604,9 +1732,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "queue_map_max")) {
 		len = num_arg(&user_buffer[i], 5, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		pkt_dev->queue_map_max = value;
 		sprintf(pg_result, "OK: queue_map_max=%u", pkt_dev->queue_map_max);
@@ -1615,10 +1743,10 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "mpls")) {
 		unsigned int n, cnt;
-
 		len = get_labels(&user_buffer[i], pkt_dev);
 		if (len < 0)
 			return len;
+
 		i += len;
 		cnt = sprintf(pg_result, "OK: mpls=");
 		for (n = 0; n < pkt_dev->nr_labels; n++)
@@ -1638,9 +1766,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "vlan_id")) {
 		len = num_arg(&user_buffer[i], 4, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (value <= 4095) {
 			pkt_dev->vlan_id = value;  /* turn on VLAN */
@@ -1665,9 +1793,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "vlan_p")) {
 		len = num_arg(&user_buffer[i], 1, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if ((value <= 7) && (pkt_dev->vlan_id != 0xffff)) {
 			pkt_dev->vlan_p = value;
@@ -1680,9 +1808,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "vlan_cfi")) {
 		len = num_arg(&user_buffer[i], 1, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if ((value <= 1) && (pkt_dev->vlan_id != 0xffff)) {
 			pkt_dev->vlan_cfi = value;
@@ -1695,9 +1823,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "svlan_id")) {
 		len = num_arg(&user_buffer[i], 4, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if ((value <= 4095) && ((pkt_dev->vlan_id != 0xffff))) {
 			pkt_dev->svlan_id = value;  /* turn on SVLAN */
@@ -1722,9 +1850,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "svlan_p")) {
 		len = num_arg(&user_buffer[i], 1, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if ((value <= 7) && (pkt_dev->svlan_id != 0xffff)) {
 			pkt_dev->svlan_p = value;
@@ -1737,9 +1865,9 @@ static ssize_t pktgen_if_write(struct file *file,
 
 	if (!strcmp(name, "svlan_cfi")) {
 		len = num_arg(&user_buffer[i], 1, &value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if ((value <= 1) && (pkt_dev->svlan_id != 0xffff)) {
 			pkt_dev->svlan_cfi = value;
@@ -1753,9 +1881,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	if (!strcmp(name, "tos")) {
 		__u32 tmp_value = 0;
 		len = hex32_arg(&user_buffer[i], 2, &tmp_value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (len == 2) {
 			pkt_dev->tos = tmp_value;
@@ -1769,9 +1897,9 @@ static ssize_t pktgen_if_write(struct file *file,
 	if (!strcmp(name, "traffic_class")) {
 		__u32 tmp_value = 0;
 		len = hex32_arg(&user_buffer[i], 2, &tmp_value);
-		if (len < 0)
+		if (len < 0) {
 			return len;
-
+		}
 		i += len;
 		if (len == 2) {
 			pkt_dev->traffic_class = tmp_value;
@@ -1794,6 +1922,7 @@ static ssize_t pktgen_if_write(struct file *file,
 		return count;
 	}
 
+	printk("pktgen: No such parameter \"%s\"\n", name);
 	sprintf(pkt_dev->result, "No such parameter \"%s\"", name);
 	return -EINVAL;
 }
@@ -1810,6 +1939,7 @@ static const struct file_operations pktgen_if_fops = {
 	.llseek  = seq_lseek,
 	.write   = pktgen_if_write,
 	.release = single_release,
+        .unlocked_ioctl   = pktgen_proc_ioctl,
 };
 
 static int pktgen_thread_show(struct seq_file *seq, void *v)
@@ -1819,16 +1949,24 @@ static int pktgen_thread_show(struct seq_file *seq, void *v)
 
 	BUG_ON(!t);
 
-	seq_puts(seq, "Running: ");
+	mutex_lock(&pktgen_thread_lock);
+	/* versioning info.  CFG_RT means we do not busy-spin, so can be configured for
+	 * real-time scheduling if user-space so desires. */
+	seq_printf(seq, "VERSION-2 CFG_RT\n");
+	seq_printf(seq, "PID: %d Name: %s  use_rel_ts: %i  nqw_callbacks:  %lu  nqw_wakeups: %lu\n",
+		   t->pid, t->tsk->comm, use_rel_ts, t->nqw_callbacks, t->nqw_wakeups);
+	seq_printf(seq, "  Sleeping: %i\n",
+		   t->sleeping);
+
+	seq_printf(seq, "Running: ");
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(pkt_dev, &t->if_list, list)
+	list_for_each_entry(pkt_dev, &t->if_list, list)
 		if (pkt_dev->running)
 			seq_printf(seq, "%s ", pkt_dev->odevname);
 
 	seq_puts(seq, "\nStopped: ");
 
-	list_for_each_entry_rcu(pkt_dev, &t->if_list, list)
+	list_for_each_entry(pkt_dev, &t->if_list, list)
 		if (!pkt_dev->running)
 			seq_printf(seq, "%s ", pkt_dev->odevname);
 
@@ -1837,8 +1975,7 @@ static int pktgen_thread_show(struct seq_file *seq, void *v)
 	else
 		seq_puts(seq, "\nResult: NA\n");
 
-	rcu_read_unlock();
-
+	mutex_unlock(&pktgen_thread_lock);
 	return 0;
 }
 
@@ -1846,16 +1983,14 @@ static ssize_t pktgen_thread_write(struct file *file,
 				   const char __user * user_buffer,
 				   size_t count, loff_t * offset)
 {
-	struct seq_file *seq = file->private_data;
+	struct seq_file *seq = (struct seq_file *)file->private_data;
 	struct pktgen_thread *t = seq->private;
 	int i, max, len, ret;
 	char name[40];
 	char *pg_result;
 
-	if (count < 1) {
-		//      sprintf(pg_result, "Wrong command format");
+	if (count < 1)
 		return -EINVAL;
-	}
 
 	max = count;
 	len = count_trail_chars(user_buffer, max);
@@ -1886,7 +2021,7 @@ static ssize_t pktgen_thread_write(struct file *file,
 		pr_debug("t=%s, count=%lu\n", name, (unsigned long)count);
 
 	if (!t) {
-		pr_err("ERROR: No thread\n");
+		printk(KERN_ERR "pktgen: ERROR: No thread\n");
 		ret = -EINVAL;
 		goto out;
 	}
@@ -1905,13 +2040,32 @@ static ssize_t pktgen_thread_write(struct file *file,
 			return -EFAULT;
 		i += len;
 		mutex_lock(&pktgen_thread_lock);
-		ret = pktgen_add_device(t, f);
+		t->control_arg = f;
+		t->control |= T_ADD_DEV;
+		while (t->control & T_ADD_DEV) {
+			schedule_timeout_interruptible(msecs_to_jiffies(10));
+		}
+		t->control_arg = 0;
 		mutex_unlock(&pktgen_thread_lock);
-		if (!ret) {
-			ret = count;
-			sprintf(pg_result, "OK: add_device=%s", f);
-		} else
-			sprintf(pg_result, "ERROR: can not add device %s", f);
+		ret = count;
+		sprintf(pg_result, "OK: add_device=%s", f);
+		goto out;
+	}
+
+	if (!strcmp(name, "rem_device")) {
+		char f[32];
+		memset(f, 0, 32);
+		len = strn_len(&user_buffer[i], sizeof(f) - 1);
+		if (len < 0) {
+			ret = len;
+			goto out;
+		}
+		if (copy_from_user(f, &user_buffer[i], len))
+			return -EFAULT;
+		i += len;
+		pktgen_mark_device(t->net, f);
+		ret = count;
+		sprintf(pg_result, "OK: rem_device=%s", f);
 		goto out;
 	}
 
@@ -1919,17 +2073,21 @@ static ssize_t pktgen_thread_write(struct file *file,
 		mutex_lock(&pktgen_thread_lock);
 		t->control |= T_REMDEVALL;
 		mutex_unlock(&pktgen_thread_lock);
-		schedule_timeout_interruptible(msecs_to_jiffies(125));	/* Propagate thread->control  */
+		while (t->control & T_REMDEVALL) {
+			schedule_timeout_interruptible(msecs_to_jiffies(10));
+		}
 		ret = count;
 		sprintf(pg_result, "OK: rem_device_all");
 		goto out;
 	}
 
 	if (!strcmp(name, "max_before_softirq")) {
-		sprintf(pg_result, "OK: Note! max_before_softirq is obsoleted -- Do not use");
-		ret = count;
-		goto out;
-	}
+		 ret = count;
+                sprintf(pg_result, "ERROR: max_before_softirq no longer supported");
+                goto out;
+        }
+
+	printk("pktgen:  un-known command to pktgen_thread: -:%s:-\n", name);
 
 	ret = -EINVAL;
 out:
@@ -1948,8 +2106,10 @@ static const struct file_operations pktgen_thread_fops = {
 	.llseek  = seq_lseek,
 	.write   = pktgen_thread_write,
 	.release = single_release,
+        .unlocked_ioctl   = pktgen_proc_ioctl,
 };
 
+
 /* Think find or remove for NN */
 static struct pktgen_dev *__pktgen_NN_threads(const struct pktgen_net *pn,
 					      const char *ifname, int remove)
@@ -1981,7 +2141,7 @@ static void pktgen_mark_device(const struct pktgen_net *pn, const char *ifname)
 	int i = 0;
 
 	mutex_lock(&pktgen_thread_lock);
-	pr_debug("%s: marking %s for removal\n", __func__, ifname);
+	pr_debug("pktgen: pktgen_mark_device marking %s for removal\n", ifname);
 
 	while (1) {
 
@@ -1990,17 +2150,17 @@ static void pktgen_mark_device(const struct pktgen_net *pn, const char *ifname)
 			break;	/* success */
 
 		mutex_unlock(&pktgen_thread_lock);
-		pr_debug("%s: waiting for %s to disappear....\n",
-			 __func__, ifname);
+		pr_debug("pktgen: pktgen_mark_device waiting for %s to disappear....\n",
+			 ifname);
 		schedule_timeout_interruptible(msecs_to_jiffies(msec_per_try));
 		mutex_lock(&pktgen_thread_lock);
 
 		if (++i >= max_tries) {
-			pr_err("%s: timed out after waiting %d msec for device %s to be removed\n",
-			       __func__, msec_per_try * i, ifname);
+			printk(KERN_ERR "pktgen_mark_device: timed out after "
+			       "waiting %d msec for device %s to be removed\n",
+			       msec_per_try * i, ifname);
 			break;
 		}
-
 	}
 
 	mutex_unlock(&pktgen_thread_lock);
@@ -2010,12 +2170,9 @@ static void pktgen_change_name(const struct pktgen_net *pn, struct net_device *d
 {
 	struct pktgen_thread *t;
 
-	mutex_lock(&pktgen_thread_lock);
-
 	list_for_each_entry(t, &pn->pktgen_threads, th_list) {
 		struct pktgen_dev *pkt_dev;
 
-		if_lock(t);
 		list_for_each_entry(pkt_dev, &t->if_list, list) {
 			if (pkt_dev->odev != dev)
 				continue;
@@ -2027,13 +2184,11 @@ static void pktgen_change_name(const struct pktgen_net *pn, struct net_device *d
 							  &pktgen_if_fops,
 							  pkt_dev);
 			if (!pkt_dev->entry)
-				pr_err("can't move proc entry for '%s'\n",
-				       dev->name);
+				printk(KERN_ERR "pktgen: can't move proc "
+				       " entry for '%s'\n", dev->name);
 			break;
 		}
-		if_unlock(t);
 	}
-	mutex_unlock(&pktgen_thread_lock);
 }
 
 static int pktgen_device_event(struct notifier_block *unused,
@@ -2062,6 +2217,7 @@ static int pktgen_device_event(struct notifier_block *unused,
 	return NOTIFY_DONE;
 }
 
+
 static struct net_device *pktgen_dev_get_by_name(const struct pktgen_net *pn,
 						 struct pktgen_dev *pkt_dev,
 						 const char *ifname)
@@ -2069,8 +2225,8 @@ static struct net_device *pktgen_dev_get_by_name(const struct pktgen_net *pn,
 	char b[IFNAMSIZ+5];
 	int i;
 
-	for (i = 0; ifname[i] != '@'; i++) {
-		if (i == IFNAMSIZ)
+	for(i=0; ifname[i] != '@'; i++) {
+		if(i == IFNAMSIZ)
 			break;
 
 		b[i] = ifname[i];
@@ -2080,35 +2236,48 @@ static struct net_device *pktgen_dev_get_by_name(const struct pktgen_net *pn,
 	return dev_get_by_name(pn->net, b);
 }
 
-
 /* Associate pktgen_dev with a device. */
 
 static int pktgen_setup_dev(const struct pktgen_net *pn,
-			    struct pktgen_dev *pkt_dev, const char *ifname)
+			    struct pktgen_dev *pkt_dev, struct pktgen_thread* t)
 {
 	struct net_device *odev;
 	int err;
 
 	/* Clean old setups */
 	if (pkt_dev->odev) {
+#ifdef USE_NQW_CALLBACK
+		/* Set the nqw callback hooks */
+		rtnl_lock();
+		clear_nqw_hook(t, pkt_dev->odev);
+		rtnl_unlock();
+#endif
+		pkt_dev->odev->pkt_dev = NULL;
 		dev_put(pkt_dev->odev);
 		pkt_dev->odev = NULL;
 	}
 
-	odev = pktgen_dev_get_by_name(pn, pkt_dev, ifname);
+	odev = pktgen_dev_get_by_name(pn, pkt_dev, pkt_dev->ifname);
 	if (!odev) {
-		pr_err("no such netdevice: \"%s\"\n", ifname);
+		printk(KERN_ERR "pktgen: no such netdevice: \"%s\"\n", pkt_dev->ifname);
 		return -ENODEV;
 	}
 
 	if (odev->type != ARPHRD_ETHER) {
-		pr_err("not an ethernet device: \"%s\"\n", ifname);
+		printk(KERN_ERR "pktgen: not an ethernet device: \"%s\"\n", pkt_dev->ifname);
 		err = -EINVAL;
 	} else if (!netif_running(odev)) {
-		pr_err("device is down: \"%s\"\n", ifname);
+		printk(KERN_ERR "pktgen: device is down: \"%s\"\n", pkt_dev->ifname);
 		err = -ENETDOWN;
 	} else {
 		pkt_dev->odev = odev;
+#ifdef USE_NQW_CALLBACK
+		/* Set the nqw callback hooks */
+		rtnl_lock();
+		set_nqw_hook(t, pkt_dev->odev, GFP_ATOMIC);
+		rtnl_unlock();
+#endif
+		pkt_dev->odev->pkt_dev = pkt_dev;
 		return 0;
 	}
 
@@ -2119,12 +2288,17 @@ static int pktgen_setup_dev(const struct pktgen_net *pn,
 /* Read pkt_dev from the interface and set up internal pktgen_dev
  * structure to have the right information to create/send packets
  */
-static void pktgen_setup_inject(struct pktgen_dev *pkt_dev)
+static void pktgen_setup_inject(struct pktgen_net *pn, struct pktgen_dev *pkt_dev)
 {
 	int ntxq;
 
+	/* Try once more, just in case it works now. */
+	if (!pkt_dev->odev)
+		pktgen_setup_dev(pn, pkt_dev, pkt_dev->pg_thread);
+
 	if (!pkt_dev->odev) {
-		pr_err("ERROR: pkt_dev->odev == NULL in setup_inject\n");
+		printk(KERN_ERR "pktgen: ERROR: pkt_dev->odev == NULL in "
+		       "setup_inject.\n");
 		sprintf(pkt_dev->result,
 			"ERROR: pkt_dev->odev == NULL in setup_inject.\n");
 		return;
@@ -2132,17 +2306,20 @@ static void pktgen_setup_inject(struct pktgen_dev *pkt_dev)
 
 	/* make sure that we don't pick a non-existing transmit queue */
 	ntxq = pkt_dev->odev->real_num_tx_queues;
-
 	if (ntxq <= pkt_dev->queue_map_min) {
-		pr_warn("WARNING: Requested queue_map_min (zero-based) (%d) exceeds valid range [0 - %d] for (%d) queues on %s, resetting\n",
-			pkt_dev->queue_map_min, (ntxq ?: 1) - 1, ntxq,
-			pkt_dev->odevname);
+		printk(KERN_WARNING "pktgen: WARNING: Requested "
+		       "queue_map_min (zero-based) (%d) exceeds valid range "
+		       "[0 - %d] for (%d) queues on %s, resetting\n",
+		       pkt_dev->queue_map_min, (ntxq ?: 1)- 1, ntxq,
+		       pkt_dev->odevname);
 		pkt_dev->queue_map_min = (ntxq ?: 1) - 1;
 	}
 	if (pkt_dev->queue_map_max >= ntxq) {
-		pr_warn("WARNING: Requested queue_map_max (zero-based) (%d) exceeds valid range [0 - %d] for (%d) queues on %s, resetting\n",
-			pkt_dev->queue_map_max, (ntxq ?: 1) - 1, ntxq,
-			pkt_dev->odevname);
+		printk(KERN_WARNING "pktgen: WARNING: Requested "
+		       "queue_map_max (zero-based) (%d) exceeds valid range "
+		       "[0 - %d] for (%d) queues on %s, resetting\n",
+		       pkt_dev->queue_map_max, (ntxq ?: 1)- 1, ntxq,
+		       pkt_dev->odevname);
 		pkt_dev->queue_map_max = (ntxq ?: 1) - 1;
 	}
 
@@ -2150,9 +2327,12 @@ static void pktgen_setup_inject(struct pktgen_dev *pkt_dev)
 
 	if (is_zero_ether_addr(pkt_dev->src_mac))
 		ether_addr_copy(&(pkt_dev->hh[6]), pkt_dev->odev->dev_addr);
+	else
+		memcpy(&(pkt_dev->hh[6]), pkt_dev->src_mac, ETH_ALEN);
+
 
 	/* Set up Dest MAC */
-	ether_addr_copy(&(pkt_dev->hh[0]), pkt_dev->dst_mac);
+	ether_addr_copy(&pkt_dev->hh[0], pkt_dev->dst_mac);
 
 	if (pkt_dev->flags & F_IPV6) {
 		int i, set = 0, err = 1;
@@ -2180,8 +2360,7 @@ static void pktgen_setup_inject(struct pktgen_dev *pkt_dev)
 			 */
 
 			rcu_read_lock();
-			idev = __in6_dev_get(pkt_dev->odev);
-			if (idev) {
+			if ((idev = __in6_dev_get(pkt_dev->odev)) != NULL) {
 				struct inet6_ifaddr *ifp;
 
 				read_lock_bh(&idev->lock);
@@ -2197,7 +2376,7 @@ static void pktgen_setup_inject(struct pktgen_dev *pkt_dev)
 			}
 			rcu_read_unlock();
 			if (err)
-				pr_err("ERROR: IPv6 link address not available\n");
+				printk(KERN_ERR "pktgen: ERROR: IPv6 link address not availble.\n");
 		}
 	} else {
 		if (pkt_dev->min_pkt_size == 0) {
@@ -2246,47 +2425,238 @@ static void pktgen_setup_inject(struct pktgen_dev *pkt_dev)
 }
 
 
-static void spin(struct pktgen_dev *pkt_dev, ktime_t spin_until)
-{
-	ktime_t start_time, end_time;
-	s64 remaining;
-	struct hrtimer_sleeper t;
+#ifdef USE_NQW_CALLBACK
+/* Runs from interrupt */
+int pg_notify_queue_woken(struct net_device* dev) {
+	/* Find the thread that needs waking. */
+	struct pg_nqw_data* nqwd = ((struct pg_nqw_data*)(dev->nqw_data));
+	while (nqwd) {
+		struct pktgen_thread* t = nqwd->pg_thread;
+		t->control |= T_WAKE_BLOCKED;
+		/* It's not the end of the world if this races and we mis a wake...it will
+		   always wake up within one tick anyway.
+		*/
+		t->nqw_callbacks++;
+		if (t->sleeping) {
+			t->nqw_wakeups++;
+			wake_up_interruptible(&(t->queue));
+			t->sleeping = 0;
+		}
+		nqwd = nqwd->next;
+	}
+	return 0;
+}
 
-	hrtimer_init_on_stack(&t.timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
-	hrtimer_set_expires(&t.timer, spin_until);
+/* Must hold RTNL lock while calling this. */
+static int set_nqw_hook(struct pktgen_thread* t, struct net_device* dev, int gfp) {
+	/* The notify-queue-woken magic only works for physical
+	 * devices at this time.  So, apply hook to underlying
+	 * device.
+	 */
+	struct pg_nqw_data* nqwd;
+	ASSERT_RTNL();
+	BUG_ON(!t);
 
-	remaining = ktime_to_ns(hrtimer_expires_remaining(&t.timer));
-	if (remaining <= 0)
-		goto out;
+	if (!dev) {
+		WARN_ON(!dev);
+		return -ENODEV;
+	}
 
-	start_time = ktime_get();
-	if (remaining < 100000) {
-		/* for small delays (<100us), just loop until limit is reached */
-		do {
-			end_time = ktime_get();
-		} while (ktime_compare(end_time, spin_until) < 0);
-	} else {
-		/* see do_nanosleep */
-		hrtimer_init_sleeper(&t, current);
-		do {
-			set_current_state(TASK_INTERRUPTIBLE);
-			hrtimer_start_expires(&t.timer, HRTIMER_MODE_ABS);
+	if (dev->rtnl_link_ops && (strcmp(dev->rtnl_link_ops->kind, "macvlan") == 0)) {
+		struct macvlan_dev *vlan = netdev_priv(dev);
+		if (debug)
+			printk("pktgen: setting nqw_hook on lower mac-vlan dev: %p\n", vlan->lowerdev);
+		return set_nqw_hook(t, vlan->lowerdev, gfp);
+	}
 
-			if (likely(t.task))
-				schedule();
+	if (dev->priv_flags & IFF_802_1Q_VLAN) {
+		if (debug)
+			printk("pktgen: setting nqw_hook on real-dev of .1q vlan: %s\n", dev->name);
+		return set_nqw_hook(t, vlan_dev_real_dev(dev), gfp);
+	}
+
+	nqwd = (struct pg_nqw_data*)(dev->nqw_data);
+
+	if (nqwd) {
+		if (nqwd->magic == PG_NQW_MAGIC) {
+			while (nqwd) {
+				if (nqwd->pg_thread == t) {
+					atomic_inc(&(nqwd->nqw_ref_count));
+					if (debug)
+						printk("pktgen: Incremented nqw_ref_count: %d"
+						       "  device: %s  thread: %p\n",
+						       (int)(atomic_read(&(nqwd->nqw_ref_count))),
+						       dev->name, t);
+					return 0;
+				}
+				nqwd = nqwd->next;
+			}
+			goto new_nqwd;
+		}
+		else {
+			printk("pktgen:  WARNING:  set_nqw_hook: nqwd magic is NOT pktgen, dev: %s  magic: 0x%x\n",
+			       dev->name, nqwd->magic);
+			return 0;
+		}
+	}
+	else {
+	new_nqwd:
+		nqwd = kmalloc(sizeof(*nqwd), gfp);
+		if (nqwd) {
+			memset(nqwd, 0, sizeof(*nqwd));
+			nqwd->magic = PG_NQW_MAGIC;
+			atomic_inc(&(nqwd->nqw_ref_count));
+			nqwd->pg_thread = t;
+			nqwd->next = dev->nqw_data;
+			dev->nqw_data = nqwd;
+			dev->notify_queue_woken = pg_notify_queue_woken;
+			if (debug)
+				printk("pktgen: Added nqw callback to device: %s  thread: %p\n",
+				       dev->name, t);
+			return 0;
+		}
+		else {
+			printk("pktgen: ERROR:  could not allocate nqwd for dev: %s\n", dev->name);
+			return -ENOBUFS;
+		}
+	}
+}
+
+
+/* Must hold RTNL lock while calling this. */
+static void clear_nqw_hook(struct pktgen_thread* t, struct net_device* dev) {
+	/* The notify-queue-woken magic only works for physical
+	 * devices at this time.  So, apply hook to underlying
+	 * device.
+	 */
+	ASSERT_RTNL();
+	BUG_ON(!t);
 
-			hrtimer_cancel(&t.timer);
-		} while (t.task && pkt_dev->running && !signal_pending(current));
-		__set_current_state(TASK_RUNNING);
-		end_time = ktime_get();
+	if (dev->rtnl_link_ops && (strcmp(dev->rtnl_link_ops->kind, "macvlan") == 0)) {
+		struct macvlan_dev *vlan = netdev_priv(dev);
+		clear_nqw_hook(t, vlan->lowerdev);
+		return;
 	}
 
-	pkt_dev->idle_acc += ktime_to_ns(ktime_sub(end_time, start_time));
-out:
-	pkt_dev->next_tx = ktime_add_ns(spin_until, pkt_dev->delay);
-	destroy_hrtimer_on_stack(&t.timer);
+	if (dev->priv_flags & IFF_802_1Q_VLAN) {
+		clear_nqw_hook(t, vlan_dev_real_dev(dev));
+		return;
+	}
+
+	if (dev->nqw_data) {
+		struct pg_nqw_data* nqwd = (struct pg_nqw_data*)(dev->nqw_data);
+		struct pg_nqw_data* prev = nqwd;
+		if (nqwd->magic == PG_NQW_MAGIC) {
+			while (nqwd) {
+				if (t != nqwd->pg_thread) {
+					prev = nqwd;
+					nqwd = nqwd->next;
+				}
+				else {
+					break;
+				}
+			}
+			if (!nqwd) {
+				printk("pktgen ERROR: Counld not find nqwd for thread: %p  device: %s\n",
+				       t, dev->name);
+				return;
+			}
+
+			atomic_dec(&(nqwd->nqw_ref_count));
+
+			if (debug)
+				printk("pktgen: Decremented nqw_ref_count: %d  device: %s  thread: %p\n",
+				       (int)(atomic_read(&(nqwd->nqw_ref_count))),
+				       dev->name, t);
+
+			BUG_ON(atomic_read(&(nqwd->nqw_ref_count)) < 0);
+
+			if (atomic_read(&(nqwd->nqw_ref_count)) == 0) {
+				if (debug)
+					printk("pktgen: Removing nqw reference from device: %s  thread: %p\n",
+					       dev->name, t);
+				if (nqwd == dev->nqw_data) {
+					if (!nqwd->next) {
+						dev->notify_queue_woken = NULL;
+					}
+					dev->nqw_data = nqwd->next;
+				}
+				else {
+					prev->next = nqwd->next;
+				}
+				nqwd->next = NULL;
+				kfree(nqwd);
+			}
+		}
+		else {
+			printk("pktgen:  WARNING:  clear_nqw_hook: nqwd magic is NOT PKT-GEN, dev: %s  magic: 0x%x",
+			       dev->name, nqwd->magic);
+		}
+	}
+	else {
+		printk("pktgen:  Warning: nqw_data is null in clear_nqw_hook, dev: %s\n",
+		       dev->name);
+	}
+}
+
+#endif
+
+
+/* delay_ns is in nano-seconds */
+static void pg_nanodelay(u64 delay_ns, struct pktgen_dev* info) {
+	u64 idle_start = getRelativeCurNs();
+	u64 last_time;
+	u64 _diff;
+	u64 itmp = idle_start;
+	struct pktgen_dev *p = NULL;
+	struct pktgen_thread* t = info->pg_thread;
+
+	info->nanodelays++;
+	info->accum_delay_ns += delay_ns;
+	while (info->accum_delay_ns > PG_MAX_ACCUM_DELAY_NS) {
+		int delay_max_us;
+		int delay_min_us = info->accum_delay_ns >> 10;
+		if (delay_min_us < 50)
+			delay_min_us = 50;
+		delay_max_us = delay_min_us + 50;
+		info->sleeps++;
+		info->pg_thread->sleeping = 1;
+		if (delay_min_us < 1000 * (1000 / HZ)) {
+			usleep_range(delay_min_us, delay_max_us);
+		}
+		else {
+			wait_event_interruptible_timeout(t->queue, false, 1);
+		}
+		info->pg_thread->sleeping = 0;
+		/* will wake after one tick */
+		last_time = itmp;
+
+		/* Subtract delay from all interfaces for this thread, since all are blocked when
+		 * any are blocked.
+		 */
+		itmp = getRelativeCurNs();
+		_diff = (itmp - last_time);
+		list_for_each_entry(p, &t->if_list, list) {
+			p->accum_delay_ns -= _diff;
+			/* Limit saving up too much time... */
+			if (p->accum_delay_ns < -10000000) {
+				p->accum_delay_ns = -10000000;
+			}
+		}
+
+		/* For accounting, only charge this guy for the idle though...*/
+		info->idle_acc_ns += _diff;
+
+		/* break out if we are stopped or if we should transmit (maybe our ipg changed?) */
+		if (info->removal_mark || (itmp >= info->next_tx_ns) ||
+		    (t->control && T_WAKE_BLOCKED) ||
+		    (t->control && T_STOP)) {
+			break;
+		}
+	}/* while */
 }
 
+
 static inline void set_pkt_overhead(struct pktgen_dev *pkt_dev)
 {
 	pkt_dev->pkt_overhead = 0;
@@ -2336,6 +2706,7 @@ static void get_ipsec_sa(struct pktgen_dev *pkt_dev, int flow)
 {
 	struct xfrm_state *x = pkt_dev->flows[flow].x;
 	struct pktgen_net *pn = net_generic(dev_net(pkt_dev->odev), pg_net_id);
+
 	if (!x) {
 
 		if (pkt_dev->spi) {
@@ -2355,15 +2726,15 @@ static void get_ipsec_sa(struct pktgen_dev *pkt_dev, int flow)
 		if (x) {
 			pkt_dev->flows[flow].x = x;
 			set_pkt_overhead(pkt_dev);
-			pkt_dev->pkt_overhead += x->props.header_len;
+			pkt_dev->pkt_overhead+=x->props.header_len;
 		}
 
 	}
 }
 #endif
+
 static void set_cur_queue_map(struct pktgen_dev *pkt_dev)
 {
-
 	if (pkt_dev->flags & F_QUEUE_MAP_CPU)
 		pkt_dev->cur_queue_map = smp_processor_id();
 
@@ -2381,7 +2752,10 @@ static void set_cur_queue_map(struct pktgen_dev *pkt_dev)
 		}
 		pkt_dev->cur_queue_map = t;
 	}
-	pkt_dev->cur_queue_map  = pkt_dev->cur_queue_map % pkt_dev->odev->real_num_tx_queues;
+	if (pkt_dev->odev->real_num_tx_queues)
+		pkt_dev->cur_queue_map  = pkt_dev->cur_queue_map % pkt_dev->odev->real_num_tx_queues;
+	else
+		pkt_dev->cur_queue_map = 0;
 }
 
 /* Increment/randomize headers according to flags and current values
@@ -2460,7 +2834,7 @@ static void mod_cur_headers(struct pktgen_dev *pkt_dev)
 	}
 
 	if ((pkt_dev->flags & F_VID_RND) && (pkt_dev->vlan_id != 0xffff)) {
-		pkt_dev->vlan_id = prandom_u32() & (4096 - 1);
+		pkt_dev->vlan_id = prandom_u32() & (4096-1);
 	}
 
 	if ((pkt_dev->flags & F_SVID_RND) && (pkt_dev->svlan_id != 0xffff)) {
@@ -2494,18 +2868,24 @@ static void mod_cur_headers(struct pktgen_dev *pkt_dev)
 
 	if (!(pkt_dev->flags & F_IPV6)) {
 
-		imn = ntohl(pkt_dev->saddr_min);
-		imx = ntohl(pkt_dev->saddr_max);
-		if (imn < imx) {
+		if ((imn = ntohl(pkt_dev->saddr_min)) < (imx =
+							 ntohl(pkt_dev->
+							       saddr_max))) {
 			__u32 t;
-			if (pkt_dev->flags & F_IPSRC_RND)
-				t = prandom_u32() % (imx - imn) + imn;
+			if (pkt_dev->flags & F_IPSRC_RND) {
+				if (imx - imn) {
+					t = (prandom_u32() % (imx - imn)) + imn;
+				}
+				else {
+					t = imn;
+				}
+			}
 			else {
 				t = ntohl(pkt_dev->cur_saddr);
 				t++;
-				if (t > imx)
+				if (t > imx) {
 					t = imn;
-
+				}
 			}
 			pkt_dev->cur_saddr = htonl(t);
 		}
@@ -2519,16 +2899,25 @@ static void mod_cur_headers(struct pktgen_dev *pkt_dev)
 				__u32 t;
 				__be32 s;
 				if (pkt_dev->flags & F_IPDST_RND) {
-
-					do {
-						t = prandom_u32() %
-							(imx - imn) + imn;
+					if (imx - imn) {
+						t = (prandom_u32() % (imx - imn)) + imn;
+					}
+					else {
+						t = imn;
+					}
+					s = htonl(t);
+
+					while (ipv4_is_loopback(s) || ipv4_is_multicast(s)
+					       || ipv4_is_lbcast(s) || ipv4_is_zeronet(s)
+					       || ipv4_is_local_multicast(s)) {
+						if (imx - imn) {
+							t = (prandom_u32() % (imx - imn)) + imn;
+						}
+						else {
+							t = imn;
+						}
 						s = htonl(t);
-					} while (ipv4_is_loopback(s) ||
-						ipv4_is_multicast(s) ||
-						ipv4_is_lbcast(s) ||
-						ipv4_is_zeronet(s) ||
-						ipv4_is_local_multicast(s));
+					}
 					pkt_dev->cur_daddr = s;
 				} else {
 					t = ntohl(pkt_dev->cur_daddr);
@@ -2585,8 +2974,6 @@ static void mod_cur_headers(struct pktgen_dev *pkt_dev)
 	pkt_dev->flows[flow].count++;
 }
 
-
-#ifdef CONFIG_XFRM
 static u32 pktgen_dst_metrics[RTAX_MAX + 1] = {
 
 	[RTAX_HOPLIMIT] = 0x5, /* Set a static hoplimit */
@@ -2623,10 +3010,12 @@ static int pktgen_output_ipsec(struct sk_buff *skb, struct pktgen_dev *pkt_dev)
 		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTSTATEPROTOERROR);
 		goto error;
 	}
-	spin_lock_bh(&x->lock);
-	x->curlft.bytes += skb->len;
+
+	spin_lock(&x->lock);
+	x->curlft.bytes +=skb->len;
 	x->curlft.packets++;
-	spin_unlock_bh(&x->lock);
+	spin_unlock(&x->lock);
+
 error:
 	return err;
 }
@@ -2653,16 +3042,16 @@ static int process_ipsec(struct pktgen_dev *pkt_dev,
 		struct xfrm_state *x = pkt_dev->flows[pkt_dev->curfl].x;
 		int nhead = 0;
 		if (x) {
-			struct ethhdr *eth;
-			struct iphdr *iph;
 			int ret;
+			__u8 *eth;
+			struct iphdr *iph;
 
 			nhead = x->props.header_len - skb_headroom(skb);
-			if (nhead > 0) {
+			if (nhead >0) {
 				ret = pskb_expand_head(skb, nhead, 0, GFP_ATOMIC);
 				if (ret < 0) {
-					pr_err("Error expanding ipsec packet %d\n",
-					       ret);
+					printk(KERN_ERR "Error expanding "
+					       "ipsec packet %d\n",ret);
 					goto err;
 				}
 			}
@@ -2671,13 +3060,14 @@ static int process_ipsec(struct pktgen_dev *pkt_dev,
 			skb_pull(skb, ETH_HLEN);
 			ret = pktgen_output_ipsec(skb, pkt_dev);
 			if (ret) {
-				pr_err("Error creating ipsec packet %d\n", ret);
+				printk(KERN_ERR "Error creating ipsec "
+				       "packet %d\n",ret);
 				goto err;
 			}
 			/* restore ll */
-			eth = skb_push(skb, ETH_HLEN);
-			memcpy(eth, pkt_dev->hh, 2 * ETH_ALEN);
-			eth->h_proto = protocol;
+			eth = (__u8 *) skb_push(skb, ETH_HLEN);
+			memcpy(eth, pkt_dev->hh, 12);
+			*(u16 *) & eth[12] = protocol;
 
 			/* Update IPv4 header len as well as checksum value */
 			iph = ip_hdr(skb);
@@ -2690,14 +3080,13 @@ static int process_ipsec(struct pktgen_dev *pkt_dev,
 	kfree_skb(skb);
 	return 0;
 }
-#endif
 
 static void mpls_push(__be32 *mpls, struct pktgen_dev *pkt_dev)
 {
 	unsigned int i;
-	for (i = 0; i < pkt_dev->nr_labels; i++)
+	for (i = 0; i < pkt_dev->nr_labels; i++) {
 		*mpls++ = pkt_dev->labels[i] & ~MPLS_STACK_BOTTOM;
-
+	}
 	mpls--;
 	*mpls |= MPLS_STACK_BOTTOM;
 }
@@ -2711,25 +3100,25 @@ static inline __be16 build_tci(unsigned int id, unsigned int cfi,
 static void pktgen_finalize_skb(struct pktgen_dev *pkt_dev, struct sk_buff *skb,
 				int datalen)
 {
-	struct timeval timestamp;
 	struct pktgen_hdr *pgh;
 
-	pgh = skb_put(skb, sizeof(*pgh));
+	pkt_dev->pgh = (struct pktgen_hdr *)skb_put(skb, sizeof(*pgh));
+	pgh = pkt_dev->pgh;
 	datalen -= sizeof(*pgh);
 
 	if (pkt_dev->nfrags <= 0) {
-		skb_put_zero(skb, datalen);
+		skb_put(skb, datalen);
+		/* memset(skb_put(skb, datalen), 0, datalen); BEN */
 	} else {
 		int frags = pkt_dev->nfrags;
 		int i, len;
 		int frag_len;
 
-
 		if (frags > MAX_SKB_FRAGS)
 			frags = MAX_SKB_FRAGS;
 		len = datalen - frags * PAGE_SIZE;
 		if (len > 0) {
-			skb_put_zero(skb, len);
+			/* memset(skb_put(skb, len), 0, len); BEN */
 			datalen = frags * PAGE_SIZE;
 		}
 
@@ -2742,7 +3131,7 @@ static void pktgen_finalize_skb(struct pktgen_dev *pkt_dev, struct sk_buff *skb,
 
 				if (pkt_dev->node >= 0 && (pkt_dev->flags & F_NODE))
 					node = pkt_dev->node;
-				pkt_dev->page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+				pkt_dev->page = alloc_pages_node(node, GFP_KERNEL/* | __GFP_ZERO BEN */, 0);
 				if (!pkt_dev->page)
 					break;
 			}
@@ -2753,6 +3142,7 @@ static void pktgen_finalize_skb(struct pktgen_dev *pkt_dev, struct sk_buff *skb,
 			if (i == (frags - 1))
 				skb_frag_size_set(&skb_shinfo(skb)->frags[i],
 				    (datalen < PAGE_SIZE ? datalen : PAGE_SIZE));
+
 			else
 				skb_frag_size_set(&skb_shinfo(skb)->frags[i], frag_len);
 			datalen -= skb_frag_size(&skb_shinfo(skb)->frags[i]);
@@ -2768,25 +3158,19 @@ static void pktgen_finalize_skb(struct pktgen_dev *pkt_dev, struct sk_buff *skb,
 	 */
 	pgh->pgh_magic = htonl(PKTGEN_MAGIC);
 	pgh->seq_num = htonl(pkt_dev->seq_num);
+	pgh->conn_id = htons((unsigned short)(pkt_dev->conn_id));
 
-	if (pkt_dev->flags & F_NO_TIMESTAMP) {
-		pgh->tv_sec = 0;
-		pgh->tv_usec = 0;
-	} else {
-		do_gettimeofday(&timestamp);
-		pgh->tv_sec = htonl(timestamp.tv_sec);
-		pgh->tv_usec = htonl(timestamp.tv_usec);
-	}
+	timestamp_skb(pkt_dev, pgh);
 }
 
 static struct sk_buff *pktgen_alloc_skb(struct net_device *dev,
-					struct pktgen_dev *pkt_dev)
+					struct pktgen_dev *pkt_dev,
+					unsigned int extralen)
 {
-	unsigned int extralen = LL_RESERVED_SPACE(dev);
 	struct sk_buff *skb = NULL;
-	unsigned int size;
+	unsigned int size = pkt_dev->cur_pkt_size + 64 + extralen +
+			    pkt_dev->pkt_overhead + LL_RESERVED_SPACE(pkt_dev->odev);
 
-	size = pkt_dev->cur_pkt_size + 64 + extralen + pkt_dev->pkt_overhead;
 	if (pkt_dev->flags & F_NODE) {
 		int node = pkt_dev->node >= 0 ? pkt_dev->node : numa_node_id();
 
@@ -2799,19 +3183,64 @@ static struct sk_buff *pktgen_alloc_skb(struct net_device *dev,
 		 skb = __netdev_alloc_skb(dev, size, GFP_NOWAIT);
 	}
 
-	/* the caller pre-fetches from skb->data and reserves for the mac hdr */
 	if (likely(skb))
-		skb_reserve(skb, extralen - 16);
+		skb_reserve(skb, LL_RESERVED_SPACE(dev));
 
 	return skb;
 }
 
+static void pg_do_csum(struct pktgen_dev *pkt_dev, struct sk_buff *skb) {
+	struct iphdr *iph = ip_hdr(skb);
+	struct udphdr *uh;
+	struct net_device *odev = pkt_dev->odev;
+
+	if (pkt_dev->flags & F_TCP) {
+		struct tcphdr *th = tcp_hdr(skb);
+		unsigned int prefix_len = (unsigned int)((unsigned char*)th - skb->data);
+
+		if (odev->features & (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM)) {
+			skb->ip_summed = CHECKSUM_PARTIAL;
+			/* Subtract out IP hdr and before */
+			th->check = ~tcp_v4_check(skb->len - prefix_len, iph->saddr, iph->daddr, 0);
+			skb->csum_start = skb_transport_header(skb) - skb->head;
+			skb->csum_offset = offsetof(struct tcphdr, check);
+		} else {
+			skb->ip_summed = CHECKSUM_NONE;
+			th->check = 0;
+			skb->csum = 0;
+			th->check = tcp_v4_check(skb->len - prefix_len, iph->saddr, iph->daddr,
+						 csum_partial(th, th->doff << 2, skb->csum));
+		}
+		//pr_err("check: 0x%x  csum-start: %d  offset: %d summed: 0x%x  saddr: 0x%x  daddr: 0x%x len: %d headroom: %d tcphdr-offset: %d prefix-len: %d\n",
+		//       th->check, skb->csum_start, skb->csum_offset, skb->ip_summed, iph->saddr, iph->daddr,
+		//       skb->len, skb_headroom(skb), (unsigned int)((unsigned char*)th - skb->data), prefix_len);
+	} else {
+		if (odev->features & (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM)) {
+			skb->ip_summed = CHECKSUM_PARTIAL;
+			skb->csum = 0;
+			udp4_hwcsum(skb, iph->saddr, iph->daddr);
+		} else {
+			unsigned int offset = skb_transport_offset(skb);
+			__wsum csum = skb_checksum(skb, offset, skb->len - offset, 0);
+			uh = udp_hdr(skb);
+
+			/* add protocol-dependent pseudo-header */
+			uh->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+						      skb->len - offset, IPPROTO_UDP, csum);
+
+			if (uh->check == 0)
+				uh->check = CSUM_MANGLED_0;
+		}
+	}
+}
+
 static struct sk_buff *fill_packet_ipv4(struct net_device *odev,
 					struct pktgen_dev *pkt_dev)
 {
 	struct sk_buff *skb = NULL;
 	__u8 *eth;
-	struct udphdr *udph;
+	struct udphdr *udph = NULL;
+	struct tcphdr *tcph;
 	int datalen, iplen;
 	struct iphdr *iph;
 	__be16 protocol = htons(ETH_P_IP);
@@ -2820,7 +3249,7 @@ static struct sk_buff *fill_packet_ipv4(struct net_device *odev,
 	__be16 *vlan_encapsulated_proto = NULL;  /* packet type ID field (or len) for VLAN tag */
 	__be16 *svlan_tci = NULL;                /* Encapsulates priority and SVLAN ID */
 	__be16 *svlan_encapsulated_proto = NULL; /* packet type ID field (or len) for SVLAN tag */
-	u16 queue_map;
+	int cur_pkt_size;
 
 	if (pkt_dev->nr_labels)
 		protocol = htons(ETH_P_MPLS_UC);
@@ -2832,75 +3261,91 @@ static struct sk_buff *fill_packet_ipv4(struct net_device *odev,
 	 * fields.
 	 */
 	mod_cur_headers(pkt_dev);
-	queue_map = pkt_dev->cur_queue_map;
 
-	skb = pktgen_alloc_skb(odev, pkt_dev);
+	datalen = (odev->hard_header_len + 16) & ~0xf;
+	cur_pkt_size = pkt_dev->cur_pkt_size; /* protect against race */
+	skb = pktgen_alloc_skb(odev, pkt_dev, datalen);
 	if (!skb) {
 		sprintf(pkt_dev->result, "No memory");
 		return NULL;
 	}
+	pkt_dev->seq_num++; /* Increase the pktgen sequence number for the next packet. */
 
 	prefetchw(skb->data);
-	skb_reserve(skb, 16);
+	skb_reserve(skb, datalen);
 
 	/*  Reserve for ethernet and IP header  */
-	eth = skb_push(skb, 14);
-	mpls = skb_put(skb, pkt_dev->nr_labels * sizeof(__u32));
+	eth = (__u8 *) skb_push(skb, 14);
+	mpls = (__be32 *)skb_put(skb, pkt_dev->nr_labels*sizeof(__u32));
 	if (pkt_dev->nr_labels)
 		mpls_push(mpls, pkt_dev);
 
 	if (pkt_dev->vlan_id != 0xffff) {
 		if (pkt_dev->svlan_id != 0xffff) {
-			svlan_tci = skb_put(skb, sizeof(__be16));
+			svlan_tci = (__be16 *)skb_put(skb, sizeof(__be16));
 			*svlan_tci = build_tci(pkt_dev->svlan_id,
 					       pkt_dev->svlan_cfi,
 					       pkt_dev->svlan_p);
-			svlan_encapsulated_proto = skb_put(skb,
-							   sizeof(__be16));
+			svlan_encapsulated_proto = (__be16 *)skb_put(skb, sizeof(__be16));
 			*svlan_encapsulated_proto = htons(ETH_P_8021Q);
 		}
-		vlan_tci = skb_put(skb, sizeof(__be16));
+		vlan_tci = (__be16 *)skb_put(skb, sizeof(__be16));
 		*vlan_tci = build_tci(pkt_dev->vlan_id,
 				      pkt_dev->vlan_cfi,
 				      pkt_dev->vlan_p);
-		vlan_encapsulated_proto = skb_put(skb, sizeof(__be16));
+		vlan_encapsulated_proto = (__be16 *)skb_put(skb, sizeof(__be16));
 		*vlan_encapsulated_proto = htons(ETH_P_IP);
 	}
 
 	skb_reset_mac_header(skb);
 	skb_set_network_header(skb, skb->len);
-	iph = skb_put(skb, sizeof(struct iphdr));
+	iph = (struct iphdr *) skb_put(skb, sizeof(struct iphdr));
 
 	skb_set_transport_header(skb, skb->len);
-	udph = skb_put(skb, sizeof(struct udphdr));
-	skb_set_queue_mapping(skb, queue_map);
+
+	if (pkt_dev->flags & F_TCP) {
+		datalen = pkt_dev->cur_pkt_size - ETH_HLEN - 20 -
+			  sizeof(struct tcphdr) - pkt_dev->pkt_overhead;
+		if (datalen < sizeof(struct pktgen_hdr))
+			datalen = sizeof(struct pktgen_hdr);
+		tcph = (struct tcphdr *)skb_put(skb, sizeof(struct tcphdr));
+		memset(tcph, 0, sizeof(*tcph));
+		tcph->source = htons(pkt_dev->cur_udp_src);
+		tcph->dest = htons(pkt_dev->cur_udp_dst);
+		tcph->doff = sizeof(struct tcphdr) >> 2;
+		tcph->seq = htonl(pkt_dev->tcp_seqno);
+		pkt_dev->tcp_seqno += datalen;
+		tcph->window = htons(0x7FFF);
+		iplen = 20 + sizeof(struct tcphdr) + datalen;
+	} else {
+		/* Eth + IPh + UDPh + mpls */
+		datalen = cur_pkt_size - 14 - 20 - 8 -
+			pkt_dev->pkt_overhead;
+		if (datalen < sizeof(struct pktgen_hdr))
+			datalen = sizeof(struct pktgen_hdr);
+		udph = (struct udphdr *)skb_put(skb, sizeof(struct udphdr));
+
+		udph->source = htons(pkt_dev->cur_udp_src);
+		udph->dest = htons(pkt_dev->cur_udp_dst);
+		udph->len = htons(datalen + 8);	/* DATA + udphdr */
+		udph->check = 0;
+		iplen = 20 + 8 + datalen;
+	}
+
 	skb->priority = pkt_dev->skb_priority;
 
 	memcpy(eth, pkt_dev->hh, 12);
 	*(__be16 *) & eth[12] = protocol;
 
-	/* Eth + IPh + UDPh + mpls */
-	datalen = pkt_dev->cur_pkt_size - 14 - 20 - 8 -
-		  pkt_dev->pkt_overhead;
-	if (datalen < 0 || datalen < sizeof(struct pktgen_hdr))
-		datalen = sizeof(struct pktgen_hdr);
-
-	udph->source = htons(pkt_dev->cur_udp_src);
-	udph->dest = htons(pkt_dev->cur_udp_dst);
-	udph->len = htons(datalen + 8);	/* DATA + udphdr */
-	udph->check = 0;
-
 	iph->ihl = 5;
 	iph->version = 4;
 	iph->ttl = 32;
 	iph->tos = pkt_dev->tos;
-	iph->protocol = IPPROTO_UDP;	/* UDP */
+	iph->protocol = pkt_dev->flags & F_TCP ? IPPROTO_TCP : IPPROTO_UDP;
 	iph->saddr = pkt_dev->cur_saddr;
 	iph->daddr = pkt_dev->cur_daddr;
-	iph->id = htons(pkt_dev->ip_id);
-	pkt_dev->ip_id++;
+	iph->id = htons(pkt_dev->ip_id++);
 	iph->frag_off = 0;
-	iplen = 20 + 8 + datalen;
 	iph->tot_len = htons(iplen);
 	ip_send_check(iph);
 	skb->protocol = protocol;
@@ -2909,21 +3354,20 @@ static struct sk_buff *fill_packet_ipv4(struct net_device *odev,
 
 	pktgen_finalize_skb(pkt_dev, skb, datalen);
 
-	if (!(pkt_dev->flags & F_UDPCSUM)) {
-		skb->ip_summed = CHECKSUM_NONE;
-	} else if (odev->features & (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM)) {
-		skb->ip_summed = CHECKSUM_PARTIAL;
-		skb->csum = 0;
-		udp4_hwcsum(skb, iph->saddr, iph->daddr);
-	} else {
-		__wsum csum = skb_checksum(skb, skb_transport_offset(skb), datalen + 8, 0);
-
-		/* add protocol-dependent pseudo-header */
-		udph->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
-						datalen + 8, IPPROTO_UDP, csum);
+	if ((odev->mtu + ETH_HLEN) < skb->len) {
+		int hdrlen = skb_transport_header(skb) - skb_mac_header(skb);
 
-		if (udph->check == 0)
-			udph->check = CSUM_MANGLED_0;
+		if (pkt_dev->flags & F_TCP) {
+			skb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;
+			hdrlen += tcp_hdrlen(skb);
+		} else {
+			skb_shinfo(skb)->gso_type = SKB_GSO_UDP;
+			hdrlen += sizeof(struct udphdr);
+		}
+		skb_shinfo(skb)->gso_size = odev->mtu - hdrlen;
+		skb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len - hdrlen, skb_shinfo(skb)->gso_size);
+	} else {
+		skb_shinfo(skb)->gso_type = 0;
 	}
 
 #ifdef CONFIG_XFRM
@@ -2931,6 +3375,11 @@ static struct sk_buff *fill_packet_ipv4(struct net_device *odev,
 		return NULL;
 #endif
 
+	if (pkt_dev->flags & F_UDPCSUM)
+		pg_do_csum(pkt_dev, skb);
+	else
+		skb->ip_summed = CHECKSUM_NONE;
+
 	return skb;
 }
 
@@ -2948,7 +3397,7 @@ static struct sk_buff *fill_packet_ipv6(struct net_device *odev,
 	__be16 *vlan_encapsulated_proto = NULL;  /* packet type ID field (or len) for VLAN tag */
 	__be16 *svlan_tci = NULL;                /* Encapsulates priority and SVLAN ID */
 	__be16 *svlan_encapsulated_proto = NULL; /* packet type ID field (or len) for SVLAN tag */
-	u16 queue_map;
+	int cur_pkt_size;
 
 	if (pkt_dev->nr_labels)
 		protocol = htons(ETH_P_MPLS_UC);
@@ -2960,9 +3409,9 @@ static struct sk_buff *fill_packet_ipv6(struct net_device *odev,
 	 * fields.
 	 */
 	mod_cur_headers(pkt_dev);
-	queue_map = pkt_dev->cur_queue_map;
 
-	skb = pktgen_alloc_skb(odev, pkt_dev);
+	cur_pkt_size = pkt_dev->cur_pkt_size;
+	skb = pktgen_alloc_skb(odev, pkt_dev, 16);
 	if (!skb) {
 		sprintf(pkt_dev->result, "No memory");
 		return NULL;
@@ -2972,43 +3421,41 @@ static struct sk_buff *fill_packet_ipv6(struct net_device *odev,
 	skb_reserve(skb, 16);
 
 	/*  Reserve for ethernet and IP header  */
-	eth = skb_push(skb, 14);
-	mpls = skb_put(skb, pkt_dev->nr_labels * sizeof(__u32));
+	eth = (__u8 *) skb_push(skb, 14);
+	mpls = (__be32 *)skb_put(skb, pkt_dev->nr_labels*sizeof(__u32));
 	if (pkt_dev->nr_labels)
 		mpls_push(mpls, pkt_dev);
 
 	if (pkt_dev->vlan_id != 0xffff) {
 		if (pkt_dev->svlan_id != 0xffff) {
-			svlan_tci = skb_put(skb, sizeof(__be16));
+			svlan_tci = (__be16 *)skb_put(skb, sizeof(__be16));
 			*svlan_tci = build_tci(pkt_dev->svlan_id,
 					       pkt_dev->svlan_cfi,
 					       pkt_dev->svlan_p);
-			svlan_encapsulated_proto = skb_put(skb,
-							   sizeof(__be16));
+			svlan_encapsulated_proto = (__be16 *)skb_put(skb, sizeof(__be16));
 			*svlan_encapsulated_proto = htons(ETH_P_8021Q);
 		}
-		vlan_tci = skb_put(skb, sizeof(__be16));
+		vlan_tci = (__be16 *)skb_put(skb, sizeof(__be16));
 		*vlan_tci = build_tci(pkt_dev->vlan_id,
 				      pkt_dev->vlan_cfi,
 				      pkt_dev->vlan_p);
-		vlan_encapsulated_proto = skb_put(skb, sizeof(__be16));
+		vlan_encapsulated_proto = (__be16 *)skb_put(skb, sizeof(__be16));
 		*vlan_encapsulated_proto = htons(ETH_P_IPV6);
 	}
 
 	skb_reset_mac_header(skb);
 	skb_set_network_header(skb, skb->len);
-	iph = skb_put(skb, sizeof(struct ipv6hdr));
+	iph = (struct ipv6hdr *) skb_put(skb, sizeof(struct ipv6hdr));
 
 	skb_set_transport_header(skb, skb->len);
-	udph = skb_put(skb, sizeof(struct udphdr));
-	skb_set_queue_mapping(skb, queue_map);
+	udph = (struct udphdr *) skb_put(skb, sizeof(struct udphdr));
 	skb->priority = pkt_dev->skb_priority;
 
 	memcpy(eth, pkt_dev->hh, 12);
-	*(__be16 *) &eth[12] = protocol;
+	*(__be16 *) & eth[12] = protocol;
 
 	/* Eth + IPh + UDPh + mpls */
-	datalen = pkt_dev->cur_pkt_size - 14 -
+	datalen = cur_pkt_size - 14 -
 		  sizeof(struct ipv6hdr) - sizeof(struct udphdr) -
 		  pkt_dev->pkt_overhead;
 
@@ -3036,7 +3483,7 @@ static struct sk_buff *fill_packet_ipv6(struct net_device *odev,
 	iph->nexthdr = IPPROTO_UDP;
 
 	iph->daddr = pkt_dev->cur_in6_daddr;
-	iph->saddr = pkt_dev->cur_in6_saddr;
+	iph->saddr =pkt_dev->cur_in6_saddr;
 
 	skb->protocol = protocol;
 	skb->dev = odev;
@@ -3073,118 +3520,375 @@ static struct sk_buff *fill_packet(struct net_device *odev,
 		return fill_packet_ipv4(odev, pkt_dev);
 }
 
-static void pktgen_clear_counters(struct pktgen_dev *pkt_dev)
-{
-	pkt_dev->seq_num = 1;
-	pkt_dev->idle_acc = 0;
-	pkt_dev->sofar = 0;
-	pkt_dev->tx_bytes = 0;
-	pkt_dev->errors = 0;
-}
 
-/* Set up structure for sending pkts, clear counters */
+static void record_latency(struct pktgen_dev* pkt_dev, int latency) {
+        /* NOTE:  Latency can be negative */
+        int div = 100;
+        int i;
+	int jit;
 
-static void pktgen_run(struct pktgen_thread *t)
-{
-	struct pktgen_dev *pkt_dev;
-	int started = 0;
+	/* If peer is local, then we can never actually have negative times.  Probable
+	 * cause is ntp or similar changing the clock while pkt is in flight.  Count this
+	 * event for debugging purposes, and set latency to zero.
+	 */
+	if (pkt_dev->flags & F_PEER_LOCAL) {
+		if (latency < 0) {
+			pkt_dev->neg_latency++;
+			latency = 0;
+		}
+	}
 
-	func_enter();
+        pkt_dev->pkts_rcvd_since_clear_lat++;
+	pkt_dev->total_lat += latency;
+
+        if (pkt_dev->pkts_rcvd_since_clear_lat < 100) {
+                div = pkt_dev->pkts_rcvd_since_clear_lat;
+                if (pkt_dev->pkts_rcvd_since_clear_lat == 1) {
+                        pkt_dev->avg_latency = latency;
+                }
+        }
+
+        if ((div + 1) == 0) {
+                pkt_dev->avg_latency = 0;
+        }
+        else {
+                pkt_dev->avg_latency = ((pkt_dev->avg_latency * div + latency) / (div + 1));
+        }
+
+        if (latency < pkt_dev->min_latency) {
+                pkt_dev->min_latency = latency;
+        }
+        if (latency > pkt_dev->max_latency) {
+                pkt_dev->max_latency = latency;
+        }
+
+        /* Place the latency in the right 'bucket' */
+        for (i = 0; i<LAT_BUCKETS_MAX; i++) {
+                if (latency < (1<<(i+1))) {
+                        pkt_dev->latency_bkts[i]++;
+                        break;
+                }
+        }
+
+	/* Calculate jitter */
+	if (latency > pkt_dev->last_rx_lat)
+		jit = latency - pkt_dev->last_rx_lat;
+	else
+		jit = pkt_dev->last_rx_lat - latency;
+	/* pkt_dev->running_jitter = pkt_dev->running_jitter * 15/16 + jit * 1/16; */
+	/* Multiply by 1024 to decrease fixed-point rounding errors */
+	pkt_dev->running_jitter = ((pkt_dev->running_jitter * 15) >> 4) + ((jit * 1024) >> 4);
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(pkt_dev, &t->if_list, list) {
+	pkt_dev->last_rx_lat = latency;
+}/* record latency */
 
-		/*
-		 * setup odev and create initial packet.
-		 */
-		pktgen_setup_inject(pkt_dev);
 
-		if (pkt_dev->odev) {
-			pktgen_clear_counters(pkt_dev);
-			pkt_dev->skb = NULL;
-			pkt_dev->started_at = pkt_dev->next_tx = ktime_get();
+/* Returns < 0 if the skb is not a pktgen buffer. */
+int pktgen_receive(struct sk_buff* skb) {
+	bool is_tcp;
 
-			set_pkt_overhead(pkt_dev);
+        /* See if we have a pktgen packet */
+	/* TODO:  Add support for detecting IPv6, TCP packets too.  This will only
+	 * catch UDP at the moment. --Ben
+	 */
+	/* printk("pktgen-rcv, skb->len: %d\n", skb->len); */
 
-			strcpy(pkt_dev->result, "Starting");
-			pkt_dev->running = 1;	/* Cranke yeself! */
-			started++;
-		} else
-			strcpy(pkt_dev->result, "Error starting");
+	/* If this is a paged skb, make sure we pull up
+	 * whatever data we need to look at. */
+	if (!pskb_may_pull(skb, 20 + 8 + sizeof(struct pktgen_hdr))) {
+		return -1;
 	}
-	rcu_read_unlock();
-	if (started)
-		t->control &= ~(T_STOP);
-}
 
-static void pktgen_stop_all_threads_ifs(struct pktgen_net *pn)
-{
-	struct pktgen_thread *t;
+        if ((skb->len >= (20 + 8 + sizeof(struct pktgen_hdr))) &&
+            (skb->protocol == __constant_htons(ETH_P_IP))) {
+		struct pktgen_hdr* pgh;
 
-	func_enter();
+                /* It's IP, and long enough, lets check the magic number.
+                 * TODO:  This is a hack not always guaranteed to catch the right
+                 * packets.
+                 */
 
-	mutex_lock(&pktgen_thread_lock);
+		/* printk("Length & protocol passed, skb->data: %p, raw: %p\n",
+		   skb->data, skb->h.raw); */
 
-	list_for_each_entry(t, &pn->pktgen_threads, th_list)
-		t->control |= T_STOP;
+                pgh = (struct pktgen_hdr*)(skb->data + 20 + 8);
 
-	mutex_unlock(&pktgen_thread_lock);
+		if (pgh->pgh_magic != __constant_ntohl(PKTGEN_MAGIC)) {
+			/* Maybe TCP packet? */
+			if (!pskb_may_pull(skb, 20 + sizeof(struct tcphdr) + sizeof(struct pktgen_hdr))) {
+				return -1;
+			}
+
+			pgh = (struct pktgen_hdr*)(skb->data + 20 + sizeof(struct tcphdr));
+			is_tcp = true;
+		}
+		else {
+			is_tcp = false;
+		}
+
+                /*
+                tmp = (char*)(skb->data);
+                for (i = 0; i<90; i++) {
+                        printk("%02hx ", tmp[i]);
+                        if (((i + 1) % 15) == 0) {
+                                printk("\n");
+                        }
+                }
+                printk("\n");
+                */
+
+                if (pgh->pgh_magic == __constant_ntohl(PKTGEN_MAGIC)) {
+                        struct net_device* dev = skb->dev;
+                        struct pktgen_dev* pkt_dev;
+                        __u32 seq = ntohl(pgh->seq_num);
+			int hdr_len = 0;
+			bool skip_seq_update = false;
+
+			/* TODO:  Need lock..maybe */
+			pkt_dev = dev->pkt_dev;
+
+                        if (!pkt_dev) {
+				return -1;
+                        }
+
+			if (likely(skb_mac_header(skb) < skb->data)) {
+				hdr_len = skb->data - skb_mac_header(skb);
+			}
+
+                        pkt_dev->pkts_rcvd++;
+			/*printk("%s:  rcvd pkt, last_seq_rcvd: %i  seq: %i  pkts_rcvd: %llu\n",
+			       pkt_dev->ifname, pkt_dev->last_seq_rcvd, seq, pkt_dev->pkts_rcvd);
+			*/
+                        pkt_dev->bytes_rcvd += (skb->len + hdr_len);
+			/* account for pre-amble and inter-frame gap, crc */
+                        pkt_dev->bytes_rcvd_ll += (skb->len + hdr_len + 24);
+
+			/* Check for bad checksums. */
+			if (pkt_dev->flags & F_UDPCSUM) {
+				if (is_tcp) {
+					if (tcp_checksum_complete(skb)) {
+						pkt_dev->rx_crc_failed++;
+						goto out_free_skb;
+					}
+				} else {
+					if (udp_lib_checksum_complete(skb)) {
+						pkt_dev->rx_crc_failed++;
+						goto out_free_skb;
+					}
+				}
+			}
+
+			if (ntohs(pgh->conn_id) != (pkt_dev->peer_conn_id & 0xFFFF)) {
+				pkt_dev->pkts_rcvd_wrong_conn++;
+				net_info_ratelimited("%s rx-wrong-dev  skb->dev: %s  pgh->seq: %u"
+						     " pgh->conn_id: 0x%hx (%u)  peer_conn_id: %u\n",
+						     pkt_dev->ifname, skb->dev->name, seq, pgh->conn_id,
+						     ntohs(pgh->conn_id), pkt_dev->peer_conn_id);
+				goto out_free_skb;
+			}
+
+                        /* Check for out-of-sequence packets */
+                        if (pkt_dev->last_seq_rcvd == seq) {
+				/*printk("%s:  got dup, last_seq_rcvd: %i  seq: %i  pkts_rcvd: %llu\n",
+				       pkt_dev->ifname, pkt_dev->last_seq_rcvd, seq, pkt_dev->pkts_rcvd);
+				*/
+                                pkt_dev->dup_rcvd++;
+                                pkt_dev->dup_since_incr++;
+				skip_seq_update = true;
+                        }
+                        else {
+				if (!(pkt_dev->flags & F_NO_TIMESTAMP)) {
+					if (pkt_dev->flags & F_USE_REL_TS) {
+						__u64 now = getRelativeCurNs();
+						__u64 txat = ntohl(pgh->tv_hi);
+						__u64 d;
+						txat = txat << 32;
+						txat |= ntohl(pgh->tv_lo);
+						d = pg_div(now - txat, 1000);
+						record_latency(pkt_dev, d);
+					}
+					else {
+						s64 tx;
+						s64 rx;
+						struct timespec rxts;
+						s64 d;
+						if (! skb->tstamp)
+							__net_timestamp(skb);
+						skb_get_timestampns(skb, &rxts);
+						rx = timespec_to_ns(&rxts);
+
+						tx = ntohl(pgh->tv_hi);
+						tx = tx << 32;
+						tx |= ntohl(pgh->tv_lo);
+						d = pg_div(rx - tx, 1000);
+						record_latency(pkt_dev, d);
+					}
+				}
+
+                                if ((pkt_dev->last_seq_rcvd + 1) == seq) {
+                                        if ((pkt_dev->peer_clone_skb > 1) &&
+                                            (pkt_dev->peer_clone_skb > (pkt_dev->dup_since_incr + 1))) {
+
+                                                pkt_dev->seq_gap_rcvd += (pkt_dev->peer_clone_skb -
+                                                                       pkt_dev->dup_since_incr - 1);
+                                        }
+                                        /* Great, in order...all is well */
+                                }
+                                else if (pkt_dev->last_seq_rcvd < seq) {
+                                        /* sequence gap, means we dropped a pkt most likely */
+                                        if (pkt_dev->peer_clone_skb > 1) {
+                                                /* We dropped more than one sequence number's worth,
+                                                 * and if we're using clone_skb, then this is quite
+                                                 * a few.  This number still will not be exact, but
+                                                 * it will be closer.
+                                                 */
+                                                pkt_dev->seq_gap_rcvd += (((seq - pkt_dev->last_seq_rcvd) *
+                                                                        pkt_dev->peer_clone_skb) -
+                                                                       pkt_dev->dup_since_incr);
+                                        }
+                                        else {
+                                                pkt_dev->seq_gap_rcvd += (seq - pkt_dev->last_seq_rcvd - 1);
+                                        }
+                                }
+                                else {
+                                        pkt_dev->ooo_rcvd++; /* out-of-order */
+					skip_seq_update = true;
+                                }
+
+                                pkt_dev->dup_since_incr = 0;
+                        }
+			if (!skip_seq_update) {
+				pkt_dev->last_seq_rcvd = seq;
+			}
+		out_free_skb:
+                        kfree_skb(skb);
+                        if (debug > 1) {
+                                printk("done with pktgen_receive, free'd pkt\n");
+                        }
+                        return 0;
+                }
+        }
+        return -1; /* Let another protocol handle it, it's not for us! */
+}/* pktgen_receive */
+
+static void pg_reset_latency_counters(struct pktgen_dev* pkt_dev) {
+        int i;
+	pkt_dev->last_rx_lat = 0;
+	pkt_dev->running_jitter = 0;
+        pkt_dev->avg_latency = 0;
+        pkt_dev->min_latency = 0x7fffffff; /* largest integer */
+        pkt_dev->max_latency = 0x80000000; /* smallest integer */
+        pkt_dev->pkts_rcvd_since_clear_lat = 0;
+	pkt_dev->total_lat = 0;
+        for (i = 0; i<LAT_BUCKETS_MAX; i++) {
+                pkt_dev->latency_bkts[i] = 0;
+        }
 }
 
-static int thread_is_running(const struct pktgen_thread *t)
-{
-	const struct pktgen_dev *pkt_dev;
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(pkt_dev, &t->if_list, list)
-		if (pkt_dev->running) {
-			rcu_read_unlock();
-			return 1;
-		}
-	rcu_read_unlock();
-	return 0;
+static void pktgen_clear_counters(struct pktgen_dev *pkt_dev, int seq_too,
+				  const char* reason) {
+	/*printk("%s clear_counters, seq_too: %i reason: %s  sofar: %llu  count: %llu\n",
+	 *       pkt_dev->ifname, seq_too, reason, pkt_dev->sofar, pkt_dev->count);
+	 */
+	pkt_dev->idle_acc_ns = 0;
+	pkt_dev->sofar = 0;
+	pkt_dev->tx_bytes = 0;
+	pkt_dev->tx_bytes_ll = 0;
+	pkt_dev->errors = 0;
+	pkt_dev->xmit_dropped = 0;
+	pkt_dev->xmit_cn = 0;
+
+        pkt_dev->ooo_rcvd = 0;
+        pkt_dev->dup_rcvd = 0;
+        pkt_dev->pkts_rcvd = 0;
+	pkt_dev->rx_crc_failed = 0;
+        pkt_dev->bytes_rcvd = 0;
+        pkt_dev->bytes_rcvd_ll = 0;
+	pkt_dev->pkts_rcvd_wrong_conn = 0;
+        pkt_dev->non_pg_pkts_rcvd = 0;
+        pkt_dev->seq_gap_rcvd = 0; /* dropped */
+
+	/* Clear some transient state */
+	pkt_dev->accum_delay_ns = 0;
+	pkt_dev->sleeps = 0;
+	pkt_dev->nanodelays = 0;
+
+        /* This is a bit of a hack, but it gets the dup counters
+         * in line so we don't have false alarms on dropped pkts.
+         */
+        if (seq_too) {
+		pkt_dev->dup_since_incr = pkt_dev->peer_clone_skb - 1;
+		pkt_dev->seq_num = 0;
+		pkt_dev->last_seq_rcvd = 0;
+        }
+
+        pg_reset_latency_counters(pkt_dev);
 }
 
-static int pktgen_wait_thread_run(struct pktgen_thread *t)
+/* Set up structure for sending pkts, clear counters */
+
+static void pktgen_run(struct pktgen_thread *t)
 {
-	while (thread_is_running(t)) {
+	struct pktgen_dev *pkt_dev;
+	int started = 0;
+
+	pr_debug("pktgen: entering pktgen_run. %p\n", t);
+
+	list_for_each_entry(pkt_dev, &t->if_list, list) {
+		/* If already running (or has completed it's allotment), then ignore. */
+		if ((! pkt_dev->running) &&
+		    ((pkt_dev->count == 0) || (pkt_dev->sofar < pkt_dev->count))) {
 
-		msleep_interruptible(100);
+			/** Clear counters before we setup the first inject.
+			 * We may have already received pkts, so don't want to clear here
+			 * after all. --Ben
+			 */
+			/* pktgen_clear_counters(pkt_dev, 1, "pktgen_run"); */
+
+			/*
+			 * setup odev and create initial packet.
+			 */
+			pktgen_setup_inject(t->net, pkt_dev);
+
+			if (pkt_dev->odev) {
+				pkt_dev->running = 1;	/* Cranke yeself! */
+				if (pkt_dev->flags & F_USE_REL_TS)
+					use_rel_ts++;
 
-		if (signal_pending(current))
-			goto signal;
+				pkt_dev->skb = NULL;
+				pkt_dev->started_at = getCurUs();
+				/* Transmit first pkt after 20ms to let listener get started. */
+				pkt_dev->next_tx_ns = getRelativeCurNs() + 20 * 1000000;
+
+				set_pkt_overhead(pkt_dev);
+
+				strcpy(pkt_dev->result, "Starting");
+				started++;
+			} else
+				strcpy(pkt_dev->result, "Error starting");
+		}
 	}
-	return 1;
-signal:
-	return 0;
 }
 
-static int pktgen_wait_all_threads_run(struct pktgen_net *pn)
+static void pktgen_stop_all_threads_ifs(struct pktgen_net *pn)
 {
 	struct pktgen_thread *t;
-	int sig = 1;
 
-	mutex_lock(&pktgen_thread_lock);
+	pr_debug("pktgen: entering pktgen_stop_all_threads_ifs.\n");
 
-	list_for_each_entry(t, &pn->pktgen_threads, th_list) {
-		sig = pktgen_wait_thread_run(t);
-		if (sig == 0)
-			break;
-	}
+	mutex_lock(&pktgen_thread_lock);
 
-	if (sig == 0)
-		list_for_each_entry(t, &pn->pktgen_threads, th_list)
-			t->control |= (T_STOP);
+	list_for_each_entry(t, &pn->pktgen_threads, th_list)
+		t->control |= T_STOP;
 
 	mutex_unlock(&pktgen_thread_lock);
-	return sig;
 }
-
-static void pktgen_run_all_threads(struct pktgen_net *pn)
-{
+static void pktgen_run_all_threads(struct pktgen_net *pn, int background) {
 	struct pktgen_thread *t;
 
-	func_enter();
+	pr_debug("pktgen: entering pktgen_run_all_threads, background: %d\n",
+		 background);
 
 	mutex_lock(&pktgen_thread_lock);
 
@@ -3193,17 +3897,18 @@ static void pktgen_run_all_threads(struct pktgen_net *pn)
 
 	mutex_unlock(&pktgen_thread_lock);
 
-	/* Propagate thread->control  */
-	schedule_timeout_interruptible(msecs_to_jiffies(125));
-
-	pktgen_wait_all_threads_run(pn);
+	/* Much harder to get rid of the if_lock if we allow this to block... */
+	if (!background) {
+		printk("ERROR:  non-background mode no longer supported.\n");
+	}
 }
 
+
 static void pktgen_reset_all_threads(struct pktgen_net *pn)
 {
 	struct pktgen_thread *t;
 
-	func_enter();
+	pr_debug("pktgen: entering pktgen_reset_all_threads.\n");
 
 	mutex_lock(&pktgen_thread_lock);
 
@@ -3212,29 +3917,38 @@ static void pktgen_reset_all_threads(struct pktgen_net *pn)
 
 	mutex_unlock(&pktgen_thread_lock);
 
-	/* Propagate thread->control  */
-	schedule_timeout_interruptible(msecs_to_jiffies(125));
-
-	pktgen_wait_all_threads_run(pn);
 }
 
+
 static void show_results(struct pktgen_dev *pkt_dev, int nr_frags)
 {
-	__u64 bps, mbps, pps;
+	__u64 total_us, bps, mbps, pps, idle;
 	char *p = pkt_dev->result;
-	ktime_t elapsed = ktime_sub(pkt_dev->stopped_at,
-				    pkt_dev->started_at);
-	ktime_t idle = ns_to_ktime(pkt_dev->idle_acc);
+
+	total_us = pkt_dev->stopped_at - pkt_dev->started_at;
+
+	idle = pkt_dev->idle_acc_ns;
+	do_div(idle, 1000);
 
 	p += sprintf(p, "OK: %llu(c%llu+d%llu) usec, %llu (%dbyte,%dfrags)\n",
-		     (unsigned long long)ktime_to_us(elapsed),
-		     (unsigned long long)ktime_to_us(ktime_sub(elapsed, idle)),
-		     (unsigned long long)ktime_to_us(idle),
+		     (unsigned long long)total_us,
+		     (unsigned long long)(total_us - idle),
+		     (unsigned long long)idle,
 		     (unsigned long long)pkt_dev->sofar,
 		     pkt_dev->cur_pkt_size, nr_frags);
 
-	pps = div64_u64(pkt_dev->sofar * NSEC_PER_SEC,
-			ktime_to_ns(elapsed));
+	pps = pkt_dev->sofar * USEC_PER_SEC;
+
+	while ((total_us >> 32) != 0) {
+		pps >>= 1;
+		total_us >>= 1;
+	}
+
+	/* Fixup total_us in case it was zero..don't want div-by-zero. */
+	if (total_us == 0)
+		total_us = 1;
+
+	do_div(pps, total_us);
 
 	bps = pps * 8 * pkt_dev->cur_pkt_size;
 
@@ -3252,53 +3966,115 @@ static int pktgen_stop_device(struct pktgen_dev *pkt_dev)
 {
 	int nr_frags = pkt_dev->skb ? skb_shinfo(pkt_dev->skb)->nr_frags : -1;
 
-	if (!pkt_dev->running) {
-		pr_warn("interface: %s is already stopped\n",
-			pkt_dev->odevname);
+	if (!pkt_dev->running)
 		return -EINVAL;
-	}
 
-	pkt_dev->running = 0;
 	kfree_skb(pkt_dev->skb);
 	pkt_dev->skb = NULL;
-	pkt_dev->stopped_at = ktime_get();
+	pkt_dev->stopped_at = getCurUs();
+	pkt_dev->running = 0;
+	if (pkt_dev->flags & F_USE_REL_TS)
+		use_rel_ts--;
 
 	show_results(pkt_dev, nr_frags);
 
 	return 0;
 }
 
-static struct pktgen_dev *next_to_run(struct pktgen_thread *t)
-{
-	struct pktgen_dev *pkt_dev, *best = NULL;
+/**  Find the adapter that needs to tx next.
+ *  We need to take the blocked adapters into account, but can't ignore
+ * them forever just in case we missed the tx-queue-wake event for some
+ * reason.
+ */
+static struct pktgen_dev *next_to_run(struct pktgen_thread *t, u64 now, u64* next_running_delay) {
+	struct pktgen_dev *pkt_dev = NULL;
+	struct pktgen_dev *best = NULL;
+	struct pktgen_dev *best_blocked = NULL;
+	struct pktgen_dev *rv = NULL;
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(pkt_dev, &t->if_list, list) {
+	list_for_each_entry(pkt_dev, &t->if_list, list) {
 		if (!pkt_dev->running)
 			continue;
-		if (best == NULL)
-			best = pkt_dev;
-		else if (ktime_compare(pkt_dev->next_tx, best->next_tx) < 0)
-			best = pkt_dev;
+		if (pkt_dev->tx_blocked) {
+			if (best_blocked == NULL)
+				best_blocked = pkt_dev;
+			else {
+				if (pkt_dev->next_tx_ns < best_blocked->next_tx_ns) {
+					best_blocked = pkt_dev;
+				}
+			}
+		}
+		else {
+			if (best == NULL)
+				best = pkt_dev;
+			else {
+				if (pkt_dev->next_tx_ns < best->next_tx_ns) {
+					best = pkt_dev;
+				}
+			}
+		}
 	}
-	rcu_read_unlock();
 
-	return best;
+	/** If we have both blocked and non-blocked, and non-blocked wants to transmit now, then
+	 * choose it.  Otherwise, just choose whoever wants to run next.
+	 */
+	if (best_blocked && best) {
+		if (((best_blocked->next_tx_ns + PG_TRY_TX_ANYWAY_NS) < now) &&
+		    (best_blocked->next_tx_ns < best->next_tx_ns)) {
+			rv = best_blocked;
+		}
+		else if (best->next_tx_ns <= now) {
+			rv = best;
+		}
+		else if (best->next_tx_ns < best_blocked->next_tx_ns) {
+			rv = best;
+		}
+		else {
+			rv = best_blocked;
+		}
+	}
+
+	if (!rv) {
+		if (best_blocked && (best_blocked->next_tx_ns < (now - PG_TRY_TX_ANYWAY_NS))) {
+			rv = best_blocked;
+		}
+	}
+	if (!rv) {
+		rv = best;
+	}
+	if (!rv) {
+		rv = best_blocked;
+	}
+
+	if (rv) {
+		/* If best is blocked, we should delay a bit */
+		if (rv->tx_blocked) {
+			*next_running_delay = PG_TRY_TX_ANYWAY_NS;
+		}
+		else {
+			if (rv->next_tx_ns <= now) {
+				*next_running_delay = 0;
+			}
+			else {
+				*next_running_delay = rv->next_tx_ns - now;
+			}
+		}
+	}
+	else {
+		*next_running_delay = 10000000; /* 10ms */
+	}
+	return rv;
 }
 
 static void pktgen_stop(struct pktgen_thread *t)
 {
 	struct pktgen_dev *pkt_dev;
 
-	func_enter();
-
-	rcu_read_lock();
+	pr_debug("pktgen: entering pktgen_stop\n");
 
-	list_for_each_entry_rcu(pkt_dev, &t->if_list, list) {
+	list_for_each_entry(pkt_dev, &t->if_list, list) {
 		pktgen_stop_device(pkt_dev);
 	}
-
-	rcu_read_unlock();
 }
 
 /*
@@ -3310,7 +4086,7 @@ static void pktgen_rem_one_if(struct pktgen_thread *t)
 	struct list_head *q, *n;
 	struct pktgen_dev *cur;
 
-	func_enter();
+	pr_debug("pktgen: entering pktgen_rem_one_if\n");
 
 	list_for_each_safe(q, n, &t->if_list) {
 		cur = list_entry(q, struct pktgen_dev, list);
@@ -3327,15 +4103,21 @@ static void pktgen_rem_one_if(struct pktgen_thread *t)
 	}
 }
 
+static void pktgen_unblock_all_ifs(struct pktgen_thread *t) {
+	struct pktgen_dev *p = NULL;;
+	list_for_each_entry(p, &t->if_list, list)
+		p->tx_blocked = 0;
+}/* wake all writers */
+
+
 static void pktgen_rem_all_ifs(struct pktgen_thread *t)
 {
 	struct list_head *q, *n;
 	struct pktgen_dev *cur;
 
-	func_enter();
-
 	/* Remove all devices, free mem */
 
+	pr_debug("pktgen: entering pktgen_rem_all_ifs\n");
 	list_for_each_safe(q, n, &t->if_list) {
 		cur = list_entry(q, struct pktgen_dev, list);
 
@@ -3349,188 +4131,228 @@ static void pktgen_rem_all_ifs(struct pktgen_thread *t)
 static void pktgen_rem_thread(struct pktgen_thread *t)
 {
 	/* Remove from the thread list */
-	remove_proc_entry(t->tsk->comm, t->net->proc_dir);
-}
-
-static void pktgen_resched(struct pktgen_dev *pkt_dev)
-{
-	ktime_t idle_start = ktime_get();
-	schedule();
-	pkt_dev->idle_acc += ktime_to_ns(ktime_sub(ktime_get(), idle_start));
-}
 
-static void pktgen_wait_for_skb(struct pktgen_dev *pkt_dev)
-{
-	ktime_t idle_start = ktime_get();
-
-	while (refcount_read(&(pkt_dev->skb->users)) != 1) {
-		if (signal_pending(current))
-			break;
-
-		if (need_resched())
-			pktgen_resched(pkt_dev);
-		else
-			cpu_relax();
-	}
-	pkt_dev->idle_acc += ktime_to_ns(ktime_sub(ktime_get(), idle_start));
+	remove_proc_entry(t->tsk->comm, t->net->proc_dir);
 }
 
-static void pktgen_xmit(struct pktgen_dev *pkt_dev)
+static void pktgen_xmit(struct pktgen_dev *pkt_dev, u64 now)
 {
+	static int do_once_hsx_wrn = 1;
 	unsigned int burst = ACCESS_ONCE(pkt_dev->burst);
 	struct net_device *odev = pkt_dev->odev;
 	struct netdev_queue *txq;
-	struct sk_buff *skb;
+	u16 queue_map;
 	int ret;
+	unsigned long burst_sofar_ns = 0;
 
-	/* If device is offline, then don't send */
-	if (unlikely(!netif_running(odev) || !netif_carrier_ok(odev))) {
-		pktgen_stop_device(pkt_dev);
-		return;
-	}
-
-	/* This is max DELAY, this has special meaning of
-	 * "never transmit"
-	 */
-	if (unlikely(pkt_dev->delay == ULLONG_MAX)) {
-		pkt_dev->next_tx = ktime_add_ns(ktime_get(), ULONG_MAX);
-		return;
-	}
+	/* printk("pktgen_xmit, pkt_dev: %s  now: %llu\n", pkt_dev->ifname, now); */
 
-	/* If no skb or clone count exhausted then get new one */
-	if (!pkt_dev->skb || (pkt_dev->last_ok &&
-			      ++pkt_dev->clone_count >= pkt_dev->clone_skb)) {
-		/* build a new pkt */
-		kfree_skb(pkt_dev->skb);
+	if (pkt_dev->delay_ns || (pkt_dev->accum_delay_ns > 0)) {
+		if (now < pkt_dev->next_tx_ns) {
+			/* Don't tx early..*/
+			pkt_dev->req_tx_early++;
+			goto out;
+		}
 
-		pkt_dev->skb = fill_packet(odev, pkt_dev);
-		if (pkt_dev->skb == NULL) {
-			pr_err("ERROR: couldn't allocate skb in fill_packet\n");
-			schedule();
-			pkt_dev->clone_count--;	/* back out increment, OOM */
-			return;
+		/* This is max DELAY, this has special meaning of
+		 * "never transmit"
+		 */
+		if (pkt_dev->delay_ns == 0x7FFFFFFF) {
+			pkt_dev->next_tx_ns = getRelativeCurNs() + pkt_dev->delay_ns;
+			goto out;
 		}
-		pkt_dev->last_pkt_size = pkt_dev->skb->len;
-		pkt_dev->clone_count = 0;	/* reset counter */
 	}
 
-	if (pkt_dev->delay && pkt_dev->last_ok)
-		spin(pkt_dev, pkt_dev->next_tx);
+	queue_map = pkt_dev->cur_queue_map;
+	BUG_ON(queue_map >= odev->num_tx_queues);
+	txq = netdev_get_tx_queue(odev, queue_map);
+
+	if (netif_xmit_frozen_or_drv_stopped(txq) ||
+	    (!netif_carrier_ok(odev)) ||
+	    need_resched()) {
+		/*printk("pktgen: xmit_frozen_or_stopped: %i (state: 0x%lx) carrier_ok: %i"
+		       "  need_resched: %i  iface: %s  queue_map: %i  num_tx_queues: %i.\n",
+		       netif_xmit_frozen_or_stopped(txq), txq->state,
+		       netif_carrier_ok(odev), need_resched(), pkt_dev->odevname,
+		       queue_map, odev->num_tx_queues);*/
+
+		pkt_dev->queue_stopped++;
+		pkt_dev->tx_blocked = 1;
+		/* change tx time to now to show work was at least attempted. */
+		pkt_dev->next_tx_ns = now;
+		if (!netif_running(odev)) {
+			pktgen_stop_device(pkt_dev);
+		}
+		goto out; /* try next interface */
+	}
+
+	if (pkt_dev->last_ok || !pkt_dev->skb || pkt_dev->force_new_skb) {
+		if ((++pkt_dev->clone_count >= pkt_dev->clone_skb)
+		    || pkt_dev->force_new_skb
+		    || (!pkt_dev->skb)) {
+			short forced = 0;
+			if (unlikely(pkt_dev->force_new_skb && pkt_dev->skb
+				     && pkt_dev->clone_count < pkt_dev->clone_skb)) {
+				/* want to keep same seq num, so decrement it before fill-pkt */
+				/* printk("%s:  force-new-skb was true, seq: %i\n",
+				       pkt_dev->ifname, pkt_dev->seq_num);
+				*/
+				forced = 1;
+				pkt_dev->seq_num--;
+			}
+			/* build a new pkt */
+			kfree_skb(pkt_dev->skb);
 
-	if (pkt_dev->xmit_mode == M_NETIF_RECEIVE) {
-		skb = pkt_dev->skb;
-		skb->protocol = eth_type_trans(skb, skb->dev);
-		refcount_add(burst, &skb->users);
-		local_bh_disable();
-		do {
-			ret = netif_receive_skb(skb);
-			if (ret == NET_RX_DROP)
-				pkt_dev->errors++;
-			pkt_dev->sofar++;
-			pkt_dev->seq_num++;
-			if (refcount_read(&skb->users) != burst) {
-				/* skb was queued by rps/rfs or taps,
-				 * so cannot reuse this skb
+			pkt_dev->skb = fill_packet(odev, pkt_dev);
+			if (pkt_dev->skb == NULL) {
+				/* printk(KERN_ERR "pktgen: ERROR: couldn't "
+				 *       "allocate skb in fill_packet.\n");
 				 */
-				WARN_ON(refcount_sub_and_test(burst - 1, &skb->users));
-				/* get out of the loop and wait
-				 * until skb is consumed
-				 */
-				break;
+				schedule();
+				if (unlikely(forced))
+					pkt_dev->seq_num++; /* back this out */
+
+				pkt_dev->clone_count--;	/* back out increment, OOM */
+				pkt_dev->oom_on_alloc_skb++;
+				goto out;
 			}
-			/* skb was 'freed' by stack, so clean few
-			 * bits and reuse it
-			 */
-			skb_reset_tc(skb);
-		} while (--burst > 0);
-		goto out; /* Skips xmit_mode M_START_XMIT */
-	} else if (pkt_dev->xmit_mode == M_QUEUE_XMIT) {
-		local_bh_disable();
-		refcount_inc(&pkt_dev->skb->users);
-
-		ret = dev_queue_xmit(pkt_dev->skb);
+			pkt_dev->last_pkt_size = pkt_dev->skb->len;
+			pkt_dev->allocated_skbs++;
+			if (likely(!forced)) {
+				pkt_dev->clone_count = 0;	/* reset counter */
+
+				if (netif_needs_gso(pkt_dev->skb, netif_skb_features(pkt_dev->skb))) {
+					pr_err("Device doesn't have necessary GSO features! netif_skb_features: %llX summed %u skb-gso: %d gso-ok: %d\n",
+					       netif_skb_features(pkt_dev->skb),
+					       pkt_dev->skb->ip_summed, skb_is_gso(pkt_dev->skb),
+					       skb_gso_ok(pkt_dev->skb, netif_skb_features(pkt_dev->skb)));
+					pktgen_stop_device(pkt_dev);
+					goto out;
+				}
+			}
+			pkt_dev->force_new_skb = 0;
+			queue_map = pkt_dev->cur_queue_map;
+		}
+	}
+
+	/*
+	 * tells skb_tx_hash() to use this tx queue.
+	 * We should reset skb->mapping before each xmit() because
+	 * xmit() might change it.
+	 */
+	skb_set_queue_mapping(pkt_dev->skb, queue_map);
+
+	BUG_ON(queue_map >= odev->num_tx_queues);
+	txq = netdev_get_tx_queue(odev, queue_map);
+
+	local_bh_disable();
+
+	HARD_TX_LOCK(odev, txq, smp_processor_id());
+
+	if (!(netif_xmit_frozen_or_stopped(txq))) {
+
+		refcount_add(burst, &pkt_dev->skb->users);
+		/* If we were blocked or had errors last time, then our skb most likely needs
+		   a timer update. */
+		if (pkt_dev->pgh && (pkt_dev->tx_blocked || !pkt_dev->last_ok)) {
+			timestamp_skb(pkt_dev, pkt_dev->pgh);
+
+			if (pkt_dev->flags & F_UDPCSUM)
+				pg_do_csum(pkt_dev, pkt_dev->skb);
+		}
+	retry_now:
+		ret = netdev_start_xmit(pkt_dev->skb, odev, txq, --burst > 0);
+		burst_sofar_ns += pkt_dev->delay_ns;
+		/* printk("%s tx skb, rv: %i  s: %llu  c: %llu\n",
+		 *      pkt_dev->ifname, ret, pkt_dev->sofar, pkt_dev->count);
+		 */
 		switch (ret) {
-		case NET_XMIT_SUCCESS:
+		case NETDEV_TX_OK:
+			pkt_dev->last_ok = 1;
 			pkt_dev->sofar++;
-			pkt_dev->seq_num++;
 			pkt_dev->tx_bytes += pkt_dev->last_pkt_size;
+			pkt_dev->tx_bytes_ll += pkt_dev->last_pkt_size + 24; /* pre-amble, frame gap, crc */
+			pkt_dev->tx_blocked = 0;
+			if (burst > 0 && !netif_xmit_frozen_or_drv_stopped(txq)) {
+				if (burst_sofar_ns < PG_MAX_ACCUM_DELAY_NS)
+					goto retry_now;
+			}
+			pkt_dev->next_tx_ns = getRelativeCurNs() + burst_sofar_ns;
 			break;
-		case NET_XMIT_DROP:
+		case NET_XMIT_DROP: /* skb has been consumed if we get these next 3 */
+			pkt_dev->xmit_dropped++;
+			goto retry_next_time;
 		case NET_XMIT_CN:
-		/* These are all valid return codes for a qdisc but
-		 * indicate packets are being dropped or will likely
-		 * be dropped soon.
-		 */
-		case NETDEV_TX_BUSY:
-		/* qdisc may call dev_hard_start_xmit directly in cases
-		 * where no queues exist e.g. loopback device, virtual
-		 * devices, etc. In this case we need to handle
-		 * NETDEV_TX_ codes.
-		 */
-		default:
-			pkt_dev->errors++;
+			pkt_dev->xmit_cn++;
+			goto retry_next_time;
+		default: /* Drivers are not supposed to return other values! */
 			net_info_ratelimited("%s xmit error: %d\n",
-					     pkt_dev->odevname, ret);
-			break;
-		}
-		goto out;
-	}
+					pkt_dev->odevname, ret);
+			/* fallthru */
+		case NETDEV_TX_BUSY:
+			/* Retry it next time */
+			if (do_once_hsx_wrn) {
+				printk(KERN_INFO "pktgen: Hard xmit error: 0x%x, driver for %s doesn't do queue-stopped quite right.\n",
+				       ret, pkt_dev->odevname);
+				printk(KERN_INFO "pktgen:  Transmit request will be retried, and this error msg will not be printed again..\n");
+				do_once_hsx_wrn = 0;
+			}
 
-	txq = skb_get_tx_queue(odev, pkt_dev->skb);
+			if (ret == NETDEV_TX_BUSY)
+				refcount_dec(&(pkt_dev->skb->users));
 
-	local_bh_disable();
+			pkt_dev->queue_stopped++;
 
-	HARD_TX_LOCK(odev, txq, smp_processor_id());
+		retry_next_time:
+			pkt_dev->errors++;
+			pkt_dev->last_ok = 0;
 
-	if (unlikely(netif_xmit_frozen_or_drv_stopped(txq))) {
-		ret = NETDEV_TX_BUSY;
+			/* Try a little later..flag us as wanting to tx, but unable.  Will try again shortly.
+			 */
+			pkt_dev->tx_blocked = 1;
+			/* change tx time to now to show work was at least attempted. */
+			pkt_dev->next_tx_ns = now;
+		}
+		if (unlikely(burst))
+			WARN_ON(refcount_sub_and_test(burst, &pkt_dev->skb->users));
+	}
+	else {			/* Retry it next time */
+		/* printk("pktgen: xmit_frozen_or_stopped: %i iface: %s  queue_map: %i.\n",
+		       netif_xmit_frozen_or_stopped(txq),
+		       pkt_dev->odevname, queue_map); */
+		pkt_dev->queue_stopped++;
 		pkt_dev->last_ok = 0;
-		goto unlock;
+		/* Try a little later..flag us as wanting to tx, but unable.  Will try again shortly.
+		 */
+		pkt_dev->tx_blocked = 1;
+		/* change tx time to now to show work was at least attempted. */
+		pkt_dev->next_tx_ns = now;
 	}
-	refcount_add(burst, &pkt_dev->skb->users);
-
-xmit_more:
-	ret = netdev_start_xmit(pkt_dev->skb, odev, txq, --burst > 0);
 
-	switch (ret) {
-	case NETDEV_TX_OK:
-		pkt_dev->last_ok = 1;
-		pkt_dev->sofar++;
-		pkt_dev->seq_num++;
-		pkt_dev->tx_bytes += pkt_dev->last_pkt_size;
-		if (burst > 0 && !netif_xmit_frozen_or_drv_stopped(txq))
-			goto xmit_more;
-		break;
-	case NET_XMIT_DROP:
-	case NET_XMIT_CN:
-		/* skb has been consumed */
-		pkt_dev->errors++;
-		break;
-	default: /* Drivers are not supposed to return other values! */
-		net_info_ratelimited("%s xmit error: %d\n",
-				     pkt_dev->odevname, ret);
-		pkt_dev->errors++;
-		/* fallthru */
-	case NETDEV_TX_BUSY:
-		/* Retry it next time */
-		refcount_dec(&(pkt_dev->skb->users));
-		pkt_dev->last_ok = 0;
-	}
-	if (unlikely(burst))
-		WARN_ON(refcount_sub_and_test(burst, &pkt_dev->skb->users));
-unlock:
 	HARD_TX_UNLOCK(odev, txq);
 
-out:
 	local_bh_enable();
 
 	/* If pkt_dev->count is zero, then run forever */
 	if ((pkt_dev->count != 0) && (pkt_dev->sofar >= pkt_dev->count)) {
-		pktgen_wait_for_skb(pkt_dev);
+		if (refcount_read(&(pkt_dev->skb->users)) != 1) {
+			u64 idle_start = getRelativeCurNs();
+			while (refcount_read(&(pkt_dev->skb->users)) != 1) {
+				if (signal_pending(current)) {
+					break;
+				}
+				schedule();
+			}
+			pkt_dev->idle_acc_ns += getRelativeCurNs() - idle_start;
+		}
 
-		/* Done with this */
-		pktgen_stop_device(pkt_dev);
+		/* Done with requested work, quiesce.  Let user-space actually
+		 * do the stopping.
+		 */
+		pkt_dev->delay_ns = 0x7FFFFFFF;
+		/*pktgen_stop_device(pkt_dev); */
 	}
+out:;
 }
 
 /*
@@ -3543,37 +4365,86 @@ static int pktgen_thread_worker(void *arg)
 	struct pktgen_thread *t = arg;
 	struct pktgen_dev *pkt_dev = NULL;
 	int cpu = t->cpu;
+	u64 now;
+	u64 next_running_delay;
 
 	BUG_ON(smp_processor_id() != cpu);
 
 	init_waitqueue_head(&t->queue);
 	complete(&t->start_done);
 
-	pr_debug("starting pktgen/%d:  pid=%d\n", cpu, task_pid_nr(current));
+	pr_debug("pktgen: starting pktgen/%d:  pid=%d\n", cpu, task_pid_nr(current));
 
 	set_freezable();
 
+	__set_current_state(TASK_RUNNING);
+
 	while (!kthread_should_stop()) {
-		pkt_dev = next_to_run(t);
+		if (t->control & T_WAKE_BLOCKED) {
+			pktgen_unblock_all_ifs(t);
+			t->control &= ~(T_WAKE_BLOCKED);
+		}
+
+		now = getRelativeCurNs();
+		pkt_dev = next_to_run(t, now, &next_running_delay);
+		/* if (pkt_dev) {
+		 *	printk("pkt_dev: %s is_blocked %i, now: %llu\n",
+		 *	       pkt_dev->ifname, pkt_dev->tx_blocked, now);
+		 *}
+		 */
 
-		if (unlikely(!pkt_dev && t->control == 0)) {
+		if (!pkt_dev &&
+		    (t->control & (T_STOP | T_RUN | T_REMDEVALL | T_REMDEV))
+		    == 0) {
 			if (t->net->pktgen_exiting)
 				break;
-			wait_event_interruptible_timeout(t->queue,
-							 t->control != 0,
-							 HZ/10);
-			try_to_freeze();
-			continue;
+			prepare_to_wait(&(t->queue), &wait,
+					TASK_INTERRUPTIBLE);
+			schedule_timeout(HZ / 10);
+			finish_wait(&(t->queue), &wait);
 		}
 
-		if (likely(pkt_dev)) {
-			pktgen_xmit(pkt_dev);
+		if (pkt_dev) {
+			if (pkt_dev->tx_blocked) {
+				/* Potentially sleep for a bit.  If the
+				 * device un-blocks, then we will be woken by the wait-queue callback.
+				 */
+				u64 tx_anyway_ns = (now - PG_TRY_TX_ANYWAY_NS);
+				if (pkt_dev->next_tx_ns > tx_anyway_ns) {
+					/* printk("pkt_dev: %s blocked, now: %llu next_tx_ns: %llu  tx_anyway_ns: %llu  next_running_delay: %lluns\n",
+					         pkt_dev->ifname, now, pkt_dev->next_tx_ns,
+					         tx_anyway_ns, next_running_delay);
+					 */
+					pg_nanodelay(min(next_running_delay, (u64)(PG_TRY_TX_ANYWAY_NS)),
+						     pkt_dev);
+					/* Maybe things have changed since we went to sleep. */
+					continue;
+				}
+				/* Been PG_TRY_TX_ANYWAY_NS, Fall through and attempt to transmit anyway. */
+			}
+
+			/* If the best to run should not run yet, then sleep (or accumulate sleep) */
+			if (now < pkt_dev->next_tx_ns) {
+				/* spin(pkt_dev, pkt_dev->next_tx_us); */
+				u64 next_ipg = pkt_dev->next_tx_ns - now;
 
-			if (need_resched())
-				pktgen_resched(pkt_dev);
-			else
-				cpu_relax();
+				/* These will not actually busy-spin now.  Will run as
+				 * much as 1ms fast, and will sleep in 1ms units, assuming
+				 * our tick is 1ms.
+				 * Unless we are using high-res timers to sleep, then we get
+				 * better granularity.
+				 */
+				pg_nanodelay(next_ipg, pkt_dev);
+				now = getRelativeCurNs();
+				if (pkt_dev->removal_mark ||
+				    (pkt_dev->pg_thread->control && T_STOP)) {
+					goto skip_tx;
+				}
+			}
+
+			pktgen_xmit(pkt_dev, now);
 		}
+	skip_tx:
 
 		if (t->control & T_STOP) {
 			pktgen_stop(t);
@@ -3585,6 +4456,11 @@ static int pktgen_thread_worker(void *arg)
 			t->control &= ~(T_RUN);
 		}
 
+		if (t->control & T_ADD_DEV) {
+			pktgen_add_device(t, (char*)(t->control_arg));
+			t->control &= ~(T_ADD_DEV);
+		}
+
 		if (t->control & T_REMDEVALL) {
 			pktgen_rem_all_ifs(t);
 			t->control &= ~(T_REMDEVALL);
@@ -3598,15 +4474,24 @@ static int pktgen_thread_worker(void *arg)
 		try_to_freeze();
 	}
 
-	pr_debug("%s stopping all device\n", t->tsk->comm);
+	set_current_state(TASK_INTERRUPTIBLE);
+
+	pr_debug("pktgen: %s stopping all device\n", t->tsk->comm);
 	pktgen_stop(t);
 
-	pr_debug("%s removing all device\n", t->tsk->comm);
+	pr_debug("pktgen: %s removing all device\n", t->tsk->comm);
 	pktgen_rem_all_ifs(t);
 
-	pr_debug("%s removing thread\n", t->tsk->comm);
+	pr_debug("pktgen: %s removing thread.\n", t->tsk->comm);
 	pktgen_rem_thread(t);
 
+	/* Wait for kthread_stop */
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	}
+	__set_current_state(TASK_RUNNING);
+
 	return 0;
 }
 
@@ -3616,8 +4501,7 @@ static struct pktgen_dev *pktgen_find_dev(struct pktgen_thread *t,
 	struct pktgen_dev *p, *pkt_dev = NULL;
 	size_t len = strlen(ifname);
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(p, &t->if_list, list)
+	list_for_each_entry(p, &t->if_list, list)
 		if (strncmp(p->odevname, ifname, len) == 0) {
 			if (p->odevname[len]) {
 				if (exact || p->odevname[len] != '@')
@@ -3626,9 +4510,6 @@ static struct pktgen_dev *pktgen_find_dev(struct pktgen_thread *t,
 			pkt_dev = p;
 			break;
 		}
-
-	rcu_read_unlock();
-	pr_debug("find_dev(%s) returning %p\n", ifname, pkt_dev);
 	return pkt_dev;
 }
 
@@ -3641,31 +4522,25 @@ static int add_dev_to_thread(struct pktgen_thread *t,
 {
 	int rv = 0;
 
-	/* This function cannot be called concurrently, as its called
-	 * under pktgen_thread_lock mutex, but it can run from
-	 * userspace on another CPU than the kthread.  The if_lock()
-	 * is used here to sync with concurrent instances of
-	 * _rem_dev_from_if_list() invoked via kthread, which is also
-	 * updating the if_list */
-	if_lock(t);
-
 	if (pkt_dev->pg_thread) {
-		pr_err("ERROR: already assigned to a thread\n");
+		printk(KERN_ERR "pktgen: ERROR: already assigned "
+		       "to a thread.\n");
 		rv = -EBUSY;
 		goto out;
 	}
 
-	pkt_dev->running = 0;
+	list_add(&pkt_dev->list, &t->if_list);
 	pkt_dev->pg_thread = t;
-	list_add_rcu(&pkt_dev->list, &t->if_list);
+	if (pkt_dev->running) {
+		pkt_dev->running = 0;
+		if (pkt_dev->flags & F_USE_REL_TS)
+			use_rel_ts--;
+	}
 
 out:
-	if_unlock(t);
 	return rv;
 }
 
-/* Called under thread lock */
-
 static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 {
 	struct pktgen_dev *pkt_dev;
@@ -3676,9 +4551,13 @@ static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 
 	pkt_dev = __pktgen_NN_threads(t->net, ifname, FIND);
 	if (pkt_dev) {
-		pr_err("ERROR: interface already used\n");
+		printk(KERN_ERR "pktgen: ERROR: interface already used.\n");
 		return -EBUSY;
 	}
+	else {
+		if (debug)
+			printk("pktgen:  Attempting to add device: %s\n", ifname);
+	}
 
 	pkt_dev = kzalloc_node(sizeof(struct pktgen_dev), GFP_KERNEL, node);
 	if (!pkt_dev)
@@ -3692,9 +4571,12 @@ static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 		return -ENOMEM;
 	}
 
+	pktgen_clear_counters(pkt_dev, 1, "pktgen_add_device");
+
 	pkt_dev->removal_mark = 0;
 	pkt_dev->nfrags = 0;
-	pkt_dev->delay = pg_delay_d;
+	pkt_dev->clone_skb = pg_clone_skb_d;
+	pkt_dev->delay_ns = pg_delay_d;
 	pkt_dev->count = pg_count_d;
 	pkt_dev->sofar = 0;
 	pkt_dev->udp_src_min = 9;	/* sink port */
@@ -3709,8 +4591,9 @@ static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 	pkt_dev->svlan_id = 0xffff;
 	pkt_dev->burst = 1;
 	pkt_dev->node = -1;
+	strncpy(pkt_dev->ifname, ifname, sizeof(pkt_dev->ifname));
 
-	err = pktgen_setup_dev(t->net, pkt_dev, ifname);
+	err = pktgen_setup_dev(t->net, pkt_dev, t);
 	if (err)
 		goto out1;
 	if (pkt_dev->odev->priv_flags & IFF_TX_SKB_SHARING)
@@ -3718,8 +4601,9 @@ static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 
 	pkt_dev->entry = proc_create_data(ifname, 0600, t->net->proc_dir,
 					  &pktgen_if_fops, pkt_dev);
+
 	if (!pkt_dev->entry) {
-		pr_err("cannot create %s/%s procfs entry\n",
+		printk(KERN_ERR "pktgen: cannot create %s/%s procfs entry.\n",
 		       PG_PROC_DIR, ifname);
 		err = -EINVAL;
 		goto out2;
@@ -3727,7 +4611,6 @@ static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 #ifdef CONFIG_XFRM
 	pkt_dev->ipsmode = XFRM_MODE_TRANSPORT;
 	pkt_dev->ipsproto = IPPROTO_ESP;
-
 	/* xfrm tunnel mode needs additional dst to extract outter
 	 * ip header protocol/ttl/id field, here creat a phony one.
 	 * instead of looking for a valid rt, which definitely hurting
@@ -3752,7 +4635,7 @@ static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 	return err;
 }
 
-static int __net_init pktgen_create_thread(int cpu, struct pktgen_net *pn)
+static int __init pktgen_create_thread(int cpu, struct pktgen_net *pn)
 {
 	struct pktgen_thread *t;
 	struct proc_dir_entry *pe;
@@ -3761,11 +4644,11 @@ static int __net_init pktgen_create_thread(int cpu, struct pktgen_net *pn)
 	t = kzalloc_node(sizeof(struct pktgen_thread), GFP_KERNEL,
 			 cpu_to_node(cpu));
 	if (!t) {
-		pr_err("ERROR: out of memory, can't create new thread\n");
+		printk(KERN_ERR "pktgen: ERROR: out of memory, can't "
+		       "create new thread.\n");
 		return -ENOMEM;
 	}
 
-	mutex_init(&t->if_lock);
 	t->cpu = cpu;
 
 	INIT_LIST_HEAD(&t->if_list);
@@ -3778,7 +4661,8 @@ static int __net_init pktgen_create_thread(int cpu, struct pktgen_net *pn)
 				   cpu_to_node(cpu),
 				   "kpktgend_%d", cpu);
 	if (IS_ERR(p)) {
-		pr_err("kernel_thread() failed for cpu %d\n", t->cpu);
+		printk(KERN_ERR "pktgen: kernel_thread() failed "
+		       "for cpu %d\n", t->cpu);
 		list_del(&t->th_list);
 		kfree(t);
 		return PTR_ERR(p);
@@ -3789,7 +4673,7 @@ static int __net_init pktgen_create_thread(int cpu, struct pktgen_net *pn)
 	pe = proc_create_data(t->tsk->comm, 0600, pn->proc_dir,
 			      &pktgen_thread_fops, t);
 	if (!pe) {
-		pr_err("cannot create %s/%s procfs entry\n",
+		printk(KERN_ERR "pktgen: cannot create %s/%s procfs entry.\n",
 		       PG_PROC_DIR, t->tsk->comm);
 		kthread_stop(p);
 		list_del(&t->th_list);
@@ -3798,7 +4682,6 @@ static int __net_init pktgen_create_thread(int cpu, struct pktgen_net *pn)
 	}
 
 	t->net = pn;
-	get_task_struct(p);
 	wake_up_process(p);
 	wait_for_completion(&t->start_done);
 
@@ -3814,47 +4697,53 @@ static void _rem_dev_from_if_list(struct pktgen_thread *t,
 	struct list_head *q, *n;
 	struct pktgen_dev *p;
 
-	if_lock(t);
 	list_for_each_safe(q, n, &t->if_list) {
 		p = list_entry(q, struct pktgen_dev, list);
 		if (p == pkt_dev)
-			list_del_rcu(&p->list);
+			list_del(&p->list);
 	}
-	if_unlock(t);
 }
 
 static int pktgen_remove_device(struct pktgen_thread *t,
 				struct pktgen_dev *pkt_dev)
 {
-	pr_debug("remove_device pkt_dev=%p\n", pkt_dev);
+	pr_debug("pktgen: remove_device pkt_dev=%p\n", pkt_dev);
 
 	if (pkt_dev->running) {
-		pr_warn("WARNING: trying to remove a running interface, stopping it now\n");
+		/*printk(KERN_WARNING "pktgen: WARNING: trying to remove a "
+		        "running interface, stopping it now.\n");
+		 */
 		pktgen_stop_device(pkt_dev);
 	}
 
 	/* Dis-associate from the interface */
 
 	if (pkt_dev->odev) {
+
+#ifdef USE_NQW_CALLBACK
+		/* Set the nqw callback hooks */
+		rtnl_lock();
+		clear_nqw_hook(t, pkt_dev->odev);
+		rtnl_unlock();
+#endif
+		pkt_dev->odev->pkt_dev = NULL;
 		dev_put(pkt_dev->odev);
 		pkt_dev->odev = NULL;
 	}
 
-	/* Remove proc before if_list entry, because add_device uses
-	 * list to determine if interface already exist, avoid race
-	 * with proc_create_data() */
-	proc_remove(pkt_dev->entry);
-
 	/* And update the thread if_list */
+
 	_rem_dev_from_if_list(t, pkt_dev);
 
+	proc_remove(pkt_dev->entry);
+
 #ifdef CONFIG_XFRM
 	free_SAs(pkt_dev);
 #endif
 	vfree(pkt_dev->flows);
 	if (pkt_dev->page)
 		put_page(pkt_dev->page);
-	kfree_rcu(pkt_dev, rcu);
+	kfree(pkt_dev);
 	return 0;
 }
 
@@ -3864,6 +4753,11 @@ static int __net_init pg_net_init(struct net *net)
 	struct proc_dir_entry *pe;
 	int cpu, ret = 0;
 
+	pr_info("sizeof report: %d, in6_addr: %d  pktgen_hdr: %i HZ: %i  TICK_NSEC: %lu net: %p\n",
+		(int)(sizeof(struct pktgen_dev_report)),
+		(int)(sizeof(struct in6_addr)), (int)(sizeof(struct pktgen_hdr)),
+		HZ, TICK_NSEC, net);
+
 	pn->net = net;
 	INIT_LIST_HEAD(&pn->pktgen_threads);
 	pn->pktgen_exiting = false;
@@ -3872,9 +4766,10 @@ static int __net_init pg_net_init(struct net *net)
 		pr_warn("cannot create /proc/net/%s\n", PG_PROC_DIR);
 		return -ENODEV;
 	}
+
 	pe = proc_create(PGCTRL, 0600, pn->proc_dir, &pktgen_fops);
 	if (pe == NULL) {
-		pr_err("cannot create %s procfs entry\n", PGCTRL);
+		pr_err("cannot create %s procfs entry.\n", PGCTRL);
 		ret = -EINVAL;
 		goto remove;
 	}
@@ -3885,7 +4780,7 @@ static int __net_init pg_net_init(struct net *net)
 		err = pktgen_create_thread(cpu, pn);
 		if (err)
 			pr_warn("Cannot create thread for cpu %d (%d)\n",
-				   cpu, err);
+				cpu, err);
 	}
 
 	if (list_empty(&pn->pktgen_threads)) {
@@ -3894,6 +4789,8 @@ static int __net_init pg_net_init(struct net *net)
 		goto remove_entry;
 	}
 
+	pr_debug("pktgen initialization complete.\n");
+
 	return 0;
 
 remove_entry:
@@ -3914,14 +4811,13 @@ static void __net_exit pg_net_exit(struct net *net)
 	pn->pktgen_exiting = true;
 
 	mutex_lock(&pktgen_thread_lock);
-	list_splice_init(&pn->pktgen_threads, &list);
+	list_splice(&pn->pktgen_threads, &list);
 	mutex_unlock(&pktgen_thread_lock);
 
 	list_for_each_safe(q, n, &list) {
 		t = list_entry(q, struct pktgen_thread, th_list);
 		list_del(&t->th_list);
 		kthread_stop(t->tsk);
-		put_task_struct(t->tsk);
 		kfree(t);
 	}
 
@@ -3939,7 +4835,6 @@ static struct pernet_operations pg_net_ops = {
 static int __init pg_init(void)
 {
 	int ret = 0;
-
 	pr_info("%s", version);
 	ret = register_pernet_subsys(&pg_net_ops);
 	if (ret)
@@ -3948,28 +4843,24 @@ static int __init pg_init(void)
 	if (ret)
 		unregister_pernet_subsys(&pg_net_ops);
 
+	handle_pktgen_hook = pktgen_receive;
 	return ret;
 }
 
 static void __exit pg_cleanup(void)
 {
+	handle_pktgen_hook = NULL;
 	unregister_netdevice_notifier(&pktgen_notifier_block);
 	unregister_pernet_subsys(&pg_net_ops);
-	/* Don't need rcu_barrier() due to use of kfree_rcu() */
 }
 
 module_init(pg_init);
 module_exit(pg_cleanup);
 
-MODULE_AUTHOR("Robert Olsson <robert.olsson@its.uu.se>");
+MODULE_AUTHOR("Robert Olsson <robert.olsson@its.uu.se");
 MODULE_DESCRIPTION("Packet Generator tool");
 MODULE_LICENSE("GPL");
-MODULE_VERSION(VERSION);
 module_param(pg_count_d, int, 0);
-MODULE_PARM_DESC(pg_count_d, "Default number of packets to inject");
 module_param(pg_delay_d, int, 0);
-MODULE_PARM_DESC(pg_delay_d, "Default delay between packets (nanoseconds)");
 module_param(pg_clone_skb_d, int, 0);
-MODULE_PARM_DESC(pg_clone_skb_d, "Default number of copies of the same packet");
 module_param(debug, int, 0);
-MODULE_PARM_DESC(debug, "Enable debugging of pktgen module");
diff --git a/net/core/pktgen.h b/net/core/pktgen.h
new file mode 100644
index 0000000..4e3022f
--- /dev/null
+++ b/net/core/pktgen.h
@@ -0,0 +1,450 @@
+/* -*-linux-c-*-
+ * $Id: candela_2.6.13.patch,v 1.3 2005/09/30 04:45:31 greear Exp $
+ * pktgen.c: Packet Generator for performance evaluation.
+ *
+ * See pktgen.c for details of changes, etc.
+*/
+
+
+#ifndef PKTGEN_H_INCLUDE_KERNEL__
+#define PKTGEN_H_INCLUDE_KERNEL__
+
+#include <linux/version.h>
+#include <linux/in6.h>
+
+/* The buckets are exponential in 'width' */
+#define LAT_BUCKETS_MAX 32
+#define PG_MAX_ACCUM_DELAY_NS (50 * 1000) /* 50 us */
+#define PG_TRY_TX_ANYWAY_NS 50000 /* try a blocked tx queue after 50 us. */
+
+#define IP_NAME_SZ 32
+#define MAX_MPLS_LABELS 16 /* This is the max label stack depth */
+#define MPLS_STACK_BOTTOM __constant_htonl(0x00000100)
+
+/* Device flag bits */
+#define F_IPSRC_RND   (1<<0)	/* IP-Src Random  */
+#define F_IPDST_RND   (1<<1)	/* IP-Dst Random  */
+#define F_UDPSRC_RND  (1<<2)	/* UDP-Src Random */
+#define F_UDPDST_RND  (1<<3)	/* UDP-Dst Random */
+#define F_MACSRC_RND  (1<<4)	/* MAC-Src Random */
+#define F_MACDST_RND  (1<<5)	/* MAC-Dst Random */
+#define F_TXSIZE_RND  (1<<6)      /* Transmit packet size is random */
+#define F_IPV6        (1<<7)	/* Interface in IPV6 Mode */
+#define F_MPLS_RND    (1<<8)	/* Random MPLS labels */
+#define F_VID_RND     (1<<9)	/* Random VLAN ID */
+#define F_SVID_RND    (1<<10)	/* Random SVLAN ID */
+#define F_FLOW_SEQ    (1<<11)	/* Sequential flows */
+#define F_IPSEC_ON    (1<<12)	/* ipsec on for flows */
+#define F_QUEUE_MAP_RND (1<<13)	/* queue map Random */
+#define F_QUEUE_MAP_CPU (1<<14)	/* queue map mirrors smp_processor_id() */
+#define F_NODE          (1<<15)	/* Node memory alloc*/
+#define F_UDPCSUM       (1<<16)	/* Include UDP checksum */
+#define F_NO_TIMESTAMP  (1<<17)	/* Don't timestamp packets (default TS) */
+
+#define F_PG_STOPPED  (1<<28)   /* Endpoint is stopped, report only */
+#define F_TCP         (1<<29)   /* Send TCP packet instead of UDP */
+#define F_USE_REL_TS  (1<<30)	/* Use relative time-stamps, ie TSC or similar */
+#define F_PEER_LOCAL  (1<<31)	/* peer endpoint is local, allows some optimizations */
+
+/* Thread control flag bits */
+#define T_TERMINATE   (1<<0)
+#define T_STOP        (1<<1)	/* Stop run */
+#define T_RUN         (1<<2)	/* Start run */
+#define T_REMDEVALL   (1<<3)	/* Remove all devs */
+#define T_REMDEV      (1<<4)	/* Remove one dev */
+#define T_WAKE_BLOCKED (1<<5)	/* Wake up all blocked net-devices. */
+#define T_ADD_DEV     (1<<6)	/* Add a device. */
+
+/* Used to help with determining the pkts on receive */
+#define PKTGEN_MAGIC 0xbe9be955
+#define PG_PROC_DIR "pktgen"
+#define PGCTRL	    "pgctrl"
+
+#define MAX_CFLOWS  65536
+
+#define VLAN_TAG_SIZE(x) ((x)->vlan_id == 0xffff ? 0 : 4)
+#define SVLAN_TAG_SIZE(x) ((x)->svlan_id == 0xffff ? 0 : 4)
+
+struct flow_state {
+	__be32 cur_daddr;
+	int count;
+#ifdef CONFIG_XFRM
+	struct xfrm_state *x;
+#endif
+	__u32 flags;
+};
+
+/* flow flag bits */
+#define F_INIT   (1<<0)		/* flow has been initialized */
+
+struct pktgen_dev {
+
+	/*
+	 * Try to keep frequent/infrequent used vars. separated.
+	 */
+	char ifname[IFNAMSIZ];
+	char result[512];
+
+	struct proc_dir_entry *entry;	/* proc file */
+	struct pktgen_thread *pg_thread;	/* the owner */
+	struct list_head list;		/* Used for chaining in the thread's run-queue */
+
+	int running;		/* if this changes to false, the test will stop */
+
+	/* If min != max, then we will either do a linear iteration, or
+	 * we will do a random selection from within the range.
+	 */
+	__u32 flags;
+	int removal_mark;	/* non-zero => the device is marked for
+				 * removal by worker thread */
+
+	__u32 min_pkt_size;	/* = ETH_ZLEN; */
+	__u32 max_pkt_size;	/* = ETH_ZLEN; */
+	int pkt_overhead;	/* overhead for MPLS, VLANs, IPSEC etc */
+	__u32 nfrags;
+	struct page *page;
+	__u64 delay_ns;          /* Delay this much between sending packets. */
+	__u64 count;		/* Default No packets to send */
+	__u64 sofar;		/* How many pkts we've sent so far */
+	__u64 tx_bytes;		/* How many bytes we've transmitted */
+	__u64 tx_bytes_ll;	/* How many bytes we've transmitted, counting lower-level framing */
+	__u64 errors;		/* Errors when trying to transmit, pkts will be re-sent */
+	__u64 xmit_dropped;     /* got NET_XMIT_DROP return value on xmit */
+	__u64 xmit_cn;          /* got NET_XMIT_CN return value on xmit */
+	__u64 nanodelays;        /* how many times have we called nano-delay on this device? */
+	__s64 accum_delay_ns;    /* Accumulated delay..when >= 1ms, we'll sleep on a wait queue. */
+	__u64 sleeps;            /* How many times have we gone to sleep on the wait queue. */
+	__u64 queue_stopped;     /* How many times was queue stopped when we tried to xmit? */
+	/* runtime counters relating to clone_skb */
+	__u64 next_tx_ns;	/* timestamp of when to tx next */
+	__u64 req_tx_early; /* requested to tx, but is too early for us to tx. */
+
+	__u64 oom_on_alloc_skb;
+	__u64 allocated_skbs;
+	__u32 clone_count;
+
+	int tx_blocked; /* Need to tx as soon as able... */
+	int last_ok;		/* Was last skb sent?
+				 * Or a failed transmit of some sort?  This will keep
+				 * sequence numbers in order, for example.
+				 */
+	__u64 started_at;	/* micro-seconds */
+	__u64 stopped_at;	/* micro-seconds */
+	__u64 idle_acc_ns; /* nano-seconds */
+	__u32 seq_num;
+
+	__u32 conn_id; /* Identifier for pkts generated by this device */
+	__u32 peer_conn_id; /* Identifier for pkts that peer generates */
+	__u32 clone_skb;		/* Use multiple SKBs during packet gen.  If this number
+				 * is greater than 1, then that many copies of the same
+				 * packet will be sent before a new packet is allocated.
+				 * For instance, if you want to send 1024 identical packets
+				 * before creating a new packet, set clone_skb to 1024.
+				 */
+	__u32 peer_clone_skb;      /* Peer (transmitter's) clone setting. */
+	__u32 force_new_skb; /** flag:  If set, will act as if clone_skb max has been reached,
+			      * except new skb will have old seq-no, and existing skb-cloned
+			      * count will not be reset.  This will force a new pkt to be generated
+			      * w/out distrurbing pkt-drop counters, etc.  Useful for when changing
+			      * pkt-sizes with a large clone-skb setting.
+			      */
+
+	char dst_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char dst_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+
+	struct in6_addr in6_saddr;
+	struct in6_addr in6_daddr;
+	struct in6_addr cur_in6_daddr;
+	struct in6_addr cur_in6_saddr;
+	/* For ranges */
+	struct in6_addr min_in6_daddr;
+	struct in6_addr max_in6_daddr;
+	struct in6_addr min_in6_saddr;
+	struct in6_addr max_in6_saddr;
+
+	/* If we're doing ranges, random or incremental, then this
+	 * defines the min/max for those ranges.
+	 */
+	__u32 saddr_min;	/* inclusive, source IP address */
+	__u32 saddr_max;	/* exclusive, source IP address */
+	__u32 daddr_min;	/* inclusive, dest IP address */
+	__u32 daddr_max;	/* exclusive, dest IP address */
+
+	__u16 udp_src_min;	/* inclusive, source UDP port */
+	__u16 udp_src_max;	/* exclusive, source UDP port */
+	__u16 udp_dst_min;	/* inclusive, dest UDP port */
+	__u16 udp_dst_max;	/* exclusive, dest UDP port */
+
+	/* DSCP + ECN */
+	__u8 tos;            /* six most significant bits of (former) IPv4 TOS are for dscp codepoint */
+	__u8 traffic_class;  /* ditto for the (former) Traffic Class in IPv6 (see RFC 3260, sec. 4) */
+
+	/* MPLS */
+	unsigned int nr_labels;	/* Depth of stack, 0 = no MPLS */
+	__be32 labels[MAX_MPLS_LABELS];
+
+
+	/* VLAN/SVLAN (802.1Q/Q-in-Q) */
+	__u8  vlan_p;
+	__u8  vlan_cfi;
+	__u16 vlan_id;  /* 0xffff means no vlan tag */
+
+	__u8  svlan_p;
+	__u8  svlan_cfi;
+	__u16 svlan_id; /* 0xffff means no svlan tag */
+
+
+	__u32 src_mac_count;	/* How many MACs to iterate through */
+	__u32 dst_mac_count;	/* How many MACs to iterate through */
+
+	unsigned char dst_mac[ETH_ALEN];
+	unsigned char src_mac[ETH_ALEN];
+
+	__u32 cur_dst_mac_offset;
+	__u32 cur_src_mac_offset;
+	__u32 cur_saddr;
+	__u32 cur_daddr;
+	__u32 tcp_seqno;
+	__u16 ip_id;
+	__u16 cur_udp_dst;
+	__u16 cur_udp_src;
+	__u16 cur_queue_map;
+	__u16 flushed_already; // Have we seen the first force_new_skb flush?
+	__u32 cur_pkt_size;
+	__u32 last_pkt_size;
+
+	__u8 hh[14];
+	/* = {
+	   0x00, 0x80, 0xC8, 0x79, 0xB3, 0xCB,
+
+	   We fill in SRC address later
+	   0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	   0x08, 0x00
+	   };
+	 */
+	__u16 pad;		/* pad out the hh struct to an even 16 bytes */
+
+	struct sk_buff *skb;	/* skb we are to transmit next, mainly used for when we
+				 * are transmitting the same one multiple times
+				 */
+	struct pktgen_hdr *pgh; /* pointer into skb where pkt-gen header lies */
+	struct net_device *odev;	/* The out-going device.  Note that the device should
+					 * have it's pg_info pointer pointing back to this
+					 * device.  This will be set when the user specifies
+					 * the out-going device name (not when the inject is
+					 * started as it used to do.)
+					 */
+	char odevname[32];
+	struct flow_state *flows;
+	unsigned int cflows;	/* Concurrent flows (config) */
+	unsigned int lflow;	/* Flow length  (config) */
+	unsigned int nflows;	/* accumulated flows (stats) */
+	unsigned int curfl;	/* current sequenced flow (state)*/
+	__u16 queue_map_min;
+	__u16 queue_map_max;
+	__u32 skb_priority;	/* skb priority field */
+	unsigned int burst;	/* number of duplicated packets to burst */
+	int node;		/* Memory node */
+
+#ifdef CONFIG_XFRM
+	__u8	ipsmode;		/* IPSEC mode (config) */
+	__u8	ipsproto;		/* IPSEC type (config) */
+	__u32	spi;
+	struct dst_entry dst;
+	struct dst_ops dstops;
+#endif
+
+	int last_rx_lat;
+	int running_jitter; /* in micro-seconds * 1024 */
+	int avg_latency; /* in micro-seconds */
+	int min_latency;
+	int max_latency;
+	__u64 latency_bkts[LAT_BUCKETS_MAX];
+	__u64 pkts_rcvd_since_clear_lat; /* with regard to clearing/resetting the latency logic */
+	__s64 total_lat; /* add all latencies together...then can divide later for over-all average */
+
+
+	/* Fields relating to receiving pkts */
+        __u32 last_seq_rcvd;
+        __u64 ooo_rcvd;  /* out-of-order packets received */
+        __u64 pkts_rcvd; /* packets received */
+	__u64 rx_crc_failed; /* pkts received with bad checksums. */
+        __u64 dup_rcvd;  /* duplicate packets received */
+        __u64 bytes_rcvd; /* total bytes received, as obtained from the skb */
+        __u64 bytes_rcvd_ll; /* total bytes received, as obtained from the skb, includes lower-level framing */
+        __u64 seq_gap_rcvd; /* how many gaps we received.  This coorelates to
+                             * dropped pkts, except perhaps in cases where we also
+                             * have re-ordered pkts.  In that case, you have to tie-break
+                             * by looking at send v/s received pkt totals for the interfaces
+                             * involved.
+                             */
+        __u64 non_pg_pkts_rcvd; /* Count how many non-pktgen skb's we are sent to check. */
+        __u64 dup_since_incr; /* How many dumplicates since the last seq number increment,
+                               * used to detect gaps when multiskb > 1
+                               */
+	__u64 pkts_rcvd_wrong_conn; /* Packets received with wrong connection id */
+	__u64 neg_latency;
+};
+
+/** Cannot make this bigger without increasing minimum ethernet frame above 60 bytes. */
+struct pktgen_hdr {
+	__u32 pgh_magic;
+	__u32 seq_num;
+	__u32 tv_hi; // top 32-bits of 64-bit nano-sec timer.
+	__u32 tv_lo; // bottom 32-bits of 64-bit nano-sec timer.
+	__u16 conn_id; // Identifier for this pktgen flow.
+} __attribute__((__packed__));
+
+
+struct pktgen_net {
+	struct net		*net;
+	struct proc_dir_entry	*proc_dir;
+	struct list_head	pktgen_threads;
+	/* This helps speed up exit since otherwise one might wait
+	 * HZ/10 for each thread.
+	 */
+	bool			pktgen_exiting;
+};
+
+
+struct pktgen_thread {
+	struct list_head if_list;	/* All device here */
+	struct list_head th_list;
+	struct task_struct* tsk;
+	int removed;
+	char result[512];
+
+	/* Field for thread to receive "posted" events terminate, stop ifs etc. */
+
+	u32 control;
+	char* control_arg;
+	int pid;
+	int cpu;
+	int sleeping;
+	unsigned long nqw_callbacks;
+	unsigned long nqw_wakeups;
+
+	wait_queue_head_t queue;
+	struct completion start_done;
+	struct pktgen_net *net;
+};
+
+struct pg_nqw_data {
+	#define PG_NQW_MAGIC 0x82743ab6
+	u32 magic;
+	struct pg_nqw_data* next;
+	atomic_t nqw_ref_count;
+	struct pktgen_thread* pg_thread;
+};
+
+struct pktgen_dev_report {
+	__u32 api_version;
+	__u32 flags;
+	__u32 min_pkt_size;
+	__u32 max_pkt_size;
+	__u32 nfrags;
+
+	__u32 clone_skb;		/* Use multiple SKBs during packet gen.  If this number
+				 * is greater than 1, then that many copies of the same
+				 * packet will be sent before a new packet is allocated.
+				 * For instance, if you want to send 1024 identical packets
+				 * before creating a new packet, set clone_skb to 1024.
+				 */
+	__u32 peer_clone_skb;      /* Peer (transmitter's) clone setting. */
+	__s32 avg_latency; /* in micro-seconds */
+	__s32 min_latency;
+	__s32 max_latency;
+
+	char thread_name[32];
+	char interface_name[32];
+	char dst_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char dst_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	unsigned char dst_mac[ETH_ALEN];
+	unsigned char src_mac[ETH_ALEN];
+	__u16 running_jitter; /* in micro-seconds */
+	__u16 burst; /* pad to 8-byte boundary */
+
+	/* If we're doing ranges, random or incremental, then this
+	 * defines the min/max for those ranges.
+	 */
+	__u32 saddr_min;	/* inclusive, source IP address */
+	__u32 saddr_max;	/* exclusive, source IP address */
+	__u32 daddr_min;	/* inclusive, dest IP address */
+	__u32 daddr_max;	/* exclusive, dest IP address */
+
+	__u16 udp_src_min;	/* inclusive, source UDP port */
+	__u16 udp_src_max;	/* exclusive, source UDP port */
+	__u16 udp_dst_min;	/* inclusive, dest UDP port */
+	__u16 udp_dst_max;	/* exclusive, dest UDP port */
+
+	/* MPLS */
+	__u32 nr_labels;	/* Depth of stack, 0 = no MPLS */
+	__be32 labels[MAX_MPLS_LABELS];
+
+	__u32 src_mac_count;	/* How many MACs to iterate through */
+	__u32 dst_mac_count;	/* How many MACs to iterate through */
+
+	__u64 nflows;	/* accumulated flows (stats) */
+	__u32 cflows;	/* Concurrent flows (config) */
+	__u32 lflow;	/* Flow length  (config) */
+
+	__u64 delay_ns; /* Delay this much between sending packets. */
+	__u64 count;  /* Default No packets to send */
+	__u64 sofar;  /* How many pkts we've sent so far */
+	__u64 tx_bytes; /* How many bytes we've transmitted */
+	__u64 errors;    /* Errors when trying to transmit, pkts will be re-sent */
+	__u64 latency_bkts[LAT_BUCKETS_MAX];
+	__u64 pkts_rcvd_since_clear_lat; /* with regard to clearing/resetting the latency logic */
+
+		/* Fields relating to receiving pkts */
+        __u64 ooo_rcvd;  /* out-of-order packets received */
+        __u64 pkts_rcvd; /* packets received */
+        __u64 dup_rcvd;  /* duplicate packets received */
+        __u64 bytes_rcvd; /* total bytes received, as obtained from the skb */
+        __u64 seq_gap_rcvd; /* how many gaps we received.  This coorelates to
+                             * dropped pkts, except perhaps in cases where we also
+                             * have re-ordered pkts.  In that case, you have to tie-break
+                             * by looking at send v/s received pkt totals for the interfaces
+                             * involved.
+                             */
+        __u64 non_pg_pkts_rcvd; /* Count how many non-pktgen skb's we are sent to check. */
+
+	struct in6_addr in6_saddr;
+	struct in6_addr in6_daddr;
+	/* For ranges */
+	struct in6_addr min_in6_daddr;
+	struct in6_addr max_in6_daddr;
+	struct in6_addr min_in6_saddr;
+	struct in6_addr max_in6_saddr;
+
+	__u64 bytes_rcvd_ll; /* total bytes received, as obtained from the skb, includes lower-level framing */
+	__u64 tx_bytes_ll; /* total bytes transmitted, as obtained from the skb, includes lower-level framing */
+	__s64 total_lat; /* add all latencies together...then can divide later for over-all average */
+	__u64 pkts_rcvd_wrong_conn; /* How many pkts received with wrong connection id? */
+	__u32 conn_id; /* reported connection ID */
+	__u32 peer_conn_id; /* reported peer connection ID */
+	__u64 rx_crc_failed; /* pkts received with bad checksums. */
+	char future_use[208]; /* Give us some room for growth w/out changing structure size */
+} __attribute__((__packed__));
+
+/* Define some IOCTLs.  Just picking random numbers, basically. */
+#define GET_PKTGEN_INTERFACE_INFO 0x7450
+struct pktgen_ioctl_info {
+        char thread_name[32];
+        char interface_name[32];
+        struct pktgen_dev_report report;
+};
+
+
+/* Defined in dev.c */
+extern int (*handle_pktgen_hook)(struct sk_buff *skb);
+
+/* Returns < 0 if the skb is not a pktgen buffer. */
+int pktgen_receive(struct sk_buff* skb);
+
+
+#endif
-- 
2.4.11

